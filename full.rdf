<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/">
    <bib:Article rdf:about="https://link.springer.com/10.1007/s11219-022-09613-1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0963-9314,%201573-1367"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Borg</foaf:surname>
                        <foaf:givenName>Markus</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Henriksson</foaf:surname>
                        <foaf:givenName>Jens</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Socha</foaf:surname>
                        <foaf:givenName>Kasper</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lennartsson</foaf:surname>
                        <foaf:givenName>Olof</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sonnsjö Lönegren</foaf:surname>
                        <foaf:givenName>Elias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bui</foaf:surname>
                        <foaf:givenName>Thanh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tomaszewski</foaf:surname>
                        <foaf:givenName>Piotr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sathyamoorthy</foaf:surname>
                        <foaf:givenName>Sankar Raman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brink</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Helali Moghadam</foaf:surname>
                        <foaf:givenName>Mahshid</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_4710"/>
        <dc:title>Ergo, SMIRK is safe: a safety case for a machine learning component in a pedestrian automatic emergency brake system</dc:title>
        <dcterms:abstract>Abstract
            Integration of machine learning (ML) components in critical applications introduces novel challenges for software certification and verification. New safety standards and technical guidelines are under development to support the safety of ML-based systems, e.g., ISO 21448 SOTIF for the automotive domain and the Assurance of Machine Learning for use in Autonomous Systems (AMLAS) framework. SOTIF and AMLAS provide high-level guidance but the details must be chiseled out for each specific case. We initiated a research project with the goal to demonstrate a complete safety case for an ML component in an open automotive system. This paper reports results from an industry-academia collaboration on safety assurance of SMIRK, an ML-based pedestrian automatic emergency braking demonstrator running in an industry-grade simulator. We demonstrate an application of AMLAS on SMIRK for a minimalistic operational design domain, i.e., we share a complete safety case for its integrated ML-based component. Finally, we report lessons learned and provide both SMIRK and the safety case under an open-source license for the research community to reuse.</dcterms:abstract>
        <dc:date>2023-03-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Ergo, SMIRK is safe</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/10.1007/s11219-022-09613-1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-03 09:39:09</dcterms:dateSubmitted>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0963-9314,%201573-1367">
        <dc:title>Software Quality Journal</dc:title>
        <dc:identifier>DOI 10.1007/s11219-022-09613-1</dc:identifier>
        <dcterms:alternative>Software Qual J</dcterms:alternative>
        <dc:identifier>ISSN 0963-9314, 1573-1367</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_4710">
        <z:itemType>attachment</z:itemType>
        <dc:title>Borg et al_2023_Ergo, SMIRK is safe.pdf</dc:title>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/1&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/3&quot;&gt;2 Background&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/3&quot;&gt;2.1 SOTIF&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/4&quot;&gt;2.2 Safety assurance using the AMLAS process&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/5&quot;&gt;2.3 Object detection and recognition using YOLO&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/6&quot;&gt;3 Related work&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/6&quot;&gt;3.1 Safety argumentation for machine learning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/8&quot;&gt;3.2 Testing of machine learning in automated vehicles&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/9&quot;&gt;4 Method: engineering research&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/10&quot;&gt;4.1 Safety evidence from Fagan inspections&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/11&quot;&gt;4.2 Presentation structure for the safety evidence&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/11&quot;&gt;5 SMIRK System Description [C]&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/13&quot;&gt;5.1 Product scope&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/14&quot;&gt;5.2 Product functions&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/15&quot;&gt;6 SMIRK system requirements&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/15&quot;&gt;6.1 System safety requirements [A]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/16&quot;&gt;6.2 Safety requirements allocated to ML component [E]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/16&quot;&gt;6.3 Machine learning safety requirements [H]&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/16&quot;&gt;6.3.1 Performance requirements&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/17&quot;&gt;6.3.2 Robustness requirements&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/18&quot;&gt;6.4 Operational design domain [B]&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/18&quot;&gt;7 SMIRK system architecture&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/18&quot;&gt;7.1 Logical view&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/19&quot;&gt;7.2 Process view&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/20&quot;&gt;8 SMIRK data management specification&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/20&quot;&gt;8.1 Data Requirements [L]&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/21&quot;&gt;8.1.1 Desideratum: relevant&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/22&quot;&gt;8.1.2 Desideratum: complete&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/22&quot;&gt;8.1.3 Desideratum: balanced&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/23&quot;&gt;8.1.4 Desideratum: accurate&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/23&quot;&gt;8.2 Data generation log [Q]&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/24&quot;&gt;8.2.1 Positive examples&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/25&quot;&gt;8.2.2 Out-of-distribution examples&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/25&quot;&gt;8.2.3 Preprocessing and data splitting&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/26&quot;&gt;9 Machine learning component specification&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/26&quot;&gt;9.1 Pedestrian detection using YOLOv5s&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/27&quot;&gt;9.2 Model development log [U]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/27&quot;&gt;9.3 OOD detection for the safety cage architecture&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/28&quot;&gt;10 SMIRK system test specification&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/29&quot;&gt;10.1 ML model testing [AA]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/31&quot;&gt;10.2 System level testing&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/31&quot;&gt;10.2.1 Operational scenarios&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/33&quot;&gt;10.2.2 System test cases&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/35&quot;&gt;11 SMIRK test results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/35&quot;&gt;11.1 Results from data testing [S]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/37&quot;&gt;11.2 Results from model testing&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/38&quot;&gt;11.2.1 Internal test results [X]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/41&quot;&gt;11.2.2 ML verification results [Z]&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/43&quot;&gt;11.3 Results from system testing [FF]&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/45&quot;&gt;11.4 Erroneous behavior log [DD]&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/45&quot;&gt;12 Lessons learned and practical advice&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/46&quot;&gt;12.1 AI engineering in the safety context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/49&quot;&gt;12.2 Reflections on industry-academia collaboration&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/50&quot;&gt;13 Limitations and threats to validity&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/50&quot;&gt;13.1 Rigor&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/51&quot;&gt;13.2 Relevance&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/52&quot;&gt;13.3 Novelty&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/52&quot;&gt;14 Conclusion and future work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/62&quot;&gt;Acknowledgements &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZUHFHGKK/63&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1910.10045">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1910.10045 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Arrieta</foaf:surname>
                        <foaf:givenName>Alejandro Barredo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Díaz-Rodríguez</foaf:surname>
                        <foaf:givenName>Natalia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Del Ser</foaf:surname>
                        <foaf:givenName>Javier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bennetot</foaf:surname>
                        <foaf:givenName>Adrien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tabik</foaf:surname>
                        <foaf:givenName>Siham</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barbado</foaf:surname>
                        <foaf:givenName>Alberto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>García</foaf:surname>
                        <foaf:givenName>Salvador</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gil-López</foaf:surname>
                        <foaf:givenName>Sergio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Molina</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Benjamins</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chatila</foaf:surname>
                        <foaf:givenName>Raja</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Herrera</foaf:surname>
                        <foaf:givenName>Francisco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_4698"/>
        <link:link rdf:resource="#item_4699"/>
        <dc:title>Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</dc:title>
        <dcterms:abstract>In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.</dcterms:abstract>
        <dc:date>2019-12-26</dc:date>
        <z:shortTitle>Explainable Artificial Intelligence (XAI)</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1910.10045</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-01-08 15:57:05</dcterms:dateSubmitted>
        <dc:description>arXiv: 1910.10045
version: 2</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_4698">
        <rdf:value>Comment: 67 pages, 13 figures, accepted for its publication in Information Fusion</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_4699">
        <z:itemType>attachment</z:itemType>
        <dc:title>Arrieta et al_2019_Explainable Artificial Intelligence (XAI) - Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.pdf</dc:title>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/2&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/4&quot;&gt;2 Explainability: What, Why, What For and How?&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/4&quot;&gt;2.1 Terminology Clarification&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/5&quot;&gt;2.2 What?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/7&quot;&gt;2.3 Why?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/7&quot;&gt;2.4 What for?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/10&quot;&gt;2.5 How?&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/10&quot;&gt;2.5.1 Levels of Transparency in Machine Learning Models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/11&quot;&gt;2.5.2 Post-hoc Explainability Techniques for Machine Learning Models&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/12&quot;&gt;3 Transparent Machine Learning Models&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/13&quot;&gt;3.1 Linear/Logistic Regression&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/14&quot;&gt;3.2 Decision Trees&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/16&quot;&gt;3.3 K-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/16&quot;&gt;3.4 Rule-based Learning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/17&quot;&gt;3.5 General Additive Models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/17&quot;&gt;3.6 Bayesian Models&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/17&quot;&gt;4 Post-hoc Explainability Techniques for Machile Learning Models: Taxonomy, Shallow Models and Deep Learning&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/18&quot;&gt;4.1 Model-agnostic Techniques for Post-hoc Explainability&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/21&quot;&gt;4.2 Post-hoc Explainability in Shallow ML Models&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/21&quot;&gt;4.2.1 Tree Ensembles, Random Forests and Multiple Classifier Systems&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/22&quot;&gt;4.2.2 Support Vector Machines&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/23&quot;&gt;4.3 Explainability in Deep Learning&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/23&quot;&gt;4.3.1 Multi-layer Neural Networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/24&quot;&gt;4.3.2 Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/27&quot;&gt;4.3.3 Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/27&quot;&gt;4.3.4 Hybrid Transparent and Black-box Methods&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/28&quot;&gt;4.4 Alternative Taxonomy of Post-hoc Explainability Techniques for Deep Learning&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/29&quot;&gt;5 XAI: Opportunities, Challenges and blackFuture Research Needs&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/30&quot;&gt;5.1 On the Tradeoff between Interpretability and Performance&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/31&quot;&gt;5.2 On the Concept and Metrics&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/32&quot;&gt;5.3 Challenges to achieve Explainable Deep Learning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/33&quot;&gt;5.4 blackExplanations for AI Security: XAI and Adversarial Machine Learning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/33&quot;&gt;5.5 XAI and Output Confidence&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/34&quot;&gt;5.6 blackXAI, Rationale Explanation, and Critical Data Studies&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/34&quot;&gt;5.7 XAI and Theory-guided Data Science&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/35&quot;&gt;5.8 Guidelines for ensuring Interpretable AI Models&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/36&quot;&gt;6 Toward Responsible AI: Principles of Artificial Intelligence, Fairness, Privacy and Data Fusion&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/36&quot;&gt;6.1 Principles of Artificial Intelligence&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/37&quot;&gt;6.2 Fairness and Accountability&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/38&quot;&gt;6.2.1 Fairness blackand Discrimination&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/40&quot;&gt;6.2.2 Accountability&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/41&quot;&gt;6.3 Privacy and Data Fusion&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/41&quot;&gt;6.3.1 Basic Levels of Data Fusion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/41&quot;&gt;6.3.2 Emerging Data Fusion Approaches&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/42&quot;&gt;6.3.3 Opportunities and Challenges in Privacy and Data Fusion under the Responsible AI Paradigm&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/45&quot;&gt;6.4 Implementing Responsible AI Principles in an Organization&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_YKLAJQLQ/45&quot;&gt;7 Conclusions and Outlook&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.aclweb.org/anthology/W19-8403">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)</dc:title>
                <dc:identifier>DOI 10.18653/v1/W19-8403</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Association for Computational Linguistics</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clinciu</foaf:surname>
                        <foaf:givenName>Miruna-Adriana</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hastie</foaf:surname>
                        <foaf:givenName>Helen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_4701"/>
        <link:link rdf:resource="#item_4702"/>
        <dc:title>A Survey of Explainable AI Terminology</dc:title>
        <dc:date>2019</dc:date>
        <z:libraryCatalog>ACLWeb</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.aclweb.org/anthology/W19-8403</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-01-29 15:40:19</dcterms:dateSubmitted>
        <dc:description>tex.ids: Clinciu2019a</dc:description>
        <bib:pages>8–13</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_4701">
        <z:itemType>attachment</z:itemType>
        <dc:title>Clinciu_Hastie_2019_A Survey of Explainable AI Terminology.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_4702">
        <z:itemType>attachment</z:itemType>
        <dc:title>Clinciu_Hastie_2019_A Survey of Explainable AI Terminology.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.aclweb.org/anthology/W19-8403.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-02-13 15:08:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1806.00069">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:title>arXiv:1806.00069 [cs, stat]</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gilpin</foaf:surname>
                        <foaf:givenName>Leilani H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bau</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>Ben Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bajwa</foaf:surname>
                        <foaf:givenName>Ayesha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Specter</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kagal</foaf:surname>
                        <foaf:givenName>Lalana</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_4704"/>
        <link:link rdf:resource="#item_4705"/>
        <dc:title>Explaining Explanations: An Overview of Interpretability of Machine Learning</dc:title>
        <dcterms:abstract>There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.</dcterms:abstract>
        <dc:date>2019-02-03</dc:date>
        <z:shortTitle>Explaining Explanations</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1806.00069</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-01-08 15:56:55</dcterms:dateSubmitted>
        <dc:description>arXiv: 1806.00069</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_4704">
        <rdf:value>Comment: The 5th IEEE International Conference on Data Science and Advanced Analytics (DSAA 2018). [Research Track]</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_4705">
        <z:itemType>attachment</z:itemType>
        <dc:title>Gilpin et al_2019_Explaining Explanations - An Overview of Interpretability of Machine Learning.pdf</dc:title>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/1&quot;&gt;I Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/2&quot;&gt;II Background and Foundational Concepts&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/2&quot;&gt;II-A What is an Explanation?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/2&quot;&gt;II-B Interpretability vs. Completeness&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/2&quot;&gt;II-C Explainability of Deep Networks&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/3&quot;&gt;III Review&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/3&quot;&gt;III-A Explanations of Deep Network Processing&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/3&quot;&gt;III-A1 Linear Proxy Models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/3&quot;&gt;III-A2 Decision Trees&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/3&quot;&gt;III-A3 Automatic-Rule Extraction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/3&quot;&gt;III-A4 Salience Mapping&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/4&quot;&gt;III-B Explanations of Deep Network Representations&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/4&quot;&gt;III-B1 Role of Layers&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/4&quot;&gt;III-B2 Role of Individual Units&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/4&quot;&gt;III-B3 Role of Representation Vectors&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/4&quot;&gt;III-C Explanation-Producing Systems&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/4&quot;&gt;III-C1 Attention Networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/5&quot;&gt;III-C2 Disentangled Representations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/5&quot;&gt;III-C3 Generated Explanations&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/5&quot;&gt;IV Related Work&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/5&quot;&gt;IV-A Interpretability&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/6&quot;&gt;IV-B Explainable AI for HCI&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/6&quot;&gt;IV-C Explanations for Black-Box Models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/6&quot;&gt;IV-D Explainability in Other Domains&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/7&quot;&gt;V Taxonomy&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/7&quot;&gt;VI Evaluation&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/8&quot;&gt;VI-A Processing&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/8&quot;&gt;VI-B Representation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/8&quot;&gt;VI-C Explanation-Producing&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/8&quot;&gt;VII Conclusions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_H9NNFSBF/9&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1802.01933">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal><dc:title>arXiv:1802.01933 [cs]</dc:title></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guidotti</foaf:surname>
                        <foaf:givenName>Riccardo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Monreale</foaf:surname>
                        <foaf:givenName>Anna</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ruggieri</foaf:surname>
                        <foaf:givenName>Salvatore</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Turini</foaf:surname>
                        <foaf:givenName>Franco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pedreschi</foaf:surname>
                        <foaf:givenName>Dino</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giannotti</foaf:surname>
                        <foaf:givenName>Fosca</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_4707"/>
        <link:link rdf:resource="#item_4708"/>
        <dc:title>A Survey Of Methods For Explaining Black Box Models</dc:title>
        <dcterms:abstract>In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.</dcterms:abstract>
        <dc:date>2018-06-21</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1802.01933</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-01-08 15:57:00</dcterms:dateSubmitted>
        <dc:description>arXiv: 1802.01933</dc:description>
    </bib:Article>
    <bib:Memo rdf:about="#item_4707">
        <rdf:value>&lt;p&gt;Comment: This work is currently under review on an international journal&lt;/p&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_4708">
        <z:itemType>attachment</z:itemType>
        <dc:title>Guidotti et al_2018_A Survey Of Methods For Explaining Black Box Models.pdf</dc:title>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_GT8S4NNN/1&quot;&gt;A Survey Of Methods For Explaining Black Box Models&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-4232-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>KDD '16</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-4232-2</dc:identifier>
                <dc:title>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</dc:title>
                <dc:identifier>DOI 10.1145/2939672.2939778</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                        <vcard:locality>San Francisco, California, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ribeiro</foaf:surname>
                        <foaf:givenName>Marco Tulio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>Sameer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guestrin</foaf:surname>
                        <foaf:givenName>Carlos</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_4711"/>
        <dc:title>&quot;Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier</dc:title>
        <dcterms:abstract>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</dcterms:abstract>
        <dc:date>August 13, 2016</dc:date>
        <z:shortTitle>&quot;Why Should I Trust You?</z:shortTitle>
        <z:libraryCatalog>ACM Digital Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1145/2939672.2939778</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2020-01-29</dcterms:dateSubmitted>
        <bib:pages>1135–1144</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_4711">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ribeiro et al_2016_Why Should I Trust You.pdf</dc:title>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/1&quot;&gt;2 The case for explanations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/3&quot;&gt;3 Local Interpretable Model-Agnostic Explanations&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/3&quot;&gt;3.1 Interpretable Data Representations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/3&quot;&gt;3.2 Fidelity-Interpretability Trade-off&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/3&quot;&gt;3.3 Sampling for Local Exploration&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/3&quot;&gt;3.4 Sparse Linear Explanations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/4&quot;&gt;3.5 Example 1: Text classification with SVMs&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/4&quot;&gt;3.6 Example 2: Deep networks for images&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/5&quot;&gt;4 Submodular Pick forExplaining Models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/6&quot;&gt;5 Simulated User Experiments&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/6&quot;&gt;5.1 Experiment Setup&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/6&quot;&gt;5.2 Are explanations faithful to the model?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/6&quot;&gt;5.3 Should I trust this prediction?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/7&quot;&gt;5.4 Can I trust this model?&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/7&quot;&gt;6 Evaluation with human subjects&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/7&quot;&gt;6.1 Experiment setup&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/7&quot;&gt;6.2 Can users select the best classifier?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/8&quot;&gt;6.3 Can non-experts improve a classifier?&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/8&quot;&gt;6.4 Do explanations lead to insights?&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/9&quot;&gt;7 Related Work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/10&quot;&gt;8 Conclusion and Future Work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_U7MHFGP3/10&quot;&gt;9 REFERENCES&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s11633-018-1126-y">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1476-8186,%201751-8520"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xue</foaf:surname>
                        <foaf:givenName>Jian-Ru</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fang</foaf:surname>
                        <foaf:givenName>Jian-Wu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Pu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5642"/>
        <dc:subject>Autonomous vehicle</dc:subject>
        <dc:subject>event reasoning</dc:subject>
        <dc:subject>intention prediction</dc:subject>
        <dc:subject>scene representation</dc:subject>
        <dc:subject>scene understanding</dc:subject>
        <dc:title>A Survey of Scene Understanding by Event Reasoning in Autonomous Driving</dc:title>
        <dcterms:abstract>Realizing autonomy is a hot research topic for automatic vehicles in recent years. For a long time, most of the eﬀorts to this goal concentrate on understanding the scenes surrounding the ego-vehicle (autonomous vehicle itself). By completing lowlevel vision tasks, such as detection, tracking and segmentation of the surrounding traﬃc participants, e.g., pedestrian, cyclists and vehicles, the scenes can be interpreted. However, for an autonomous vehicle, low-level vision tasks are largely insuﬃcient to give help to comprehensive scene understanding. What are and how about the past, the on-going and the future of the scene participants? This deep question actually steers the vehicles towards truly full automation, just like human beings. Based on this thoughtfulness, this paper attempts to investigate the interpretation of traﬃc scene in autonomous driving from an event reasoning view. To reach this goal, we study the most relevant literatures and the state-of-the-arts on scene representation, event detection and intention prediction in autonomous driving. In addition, we also discuss the open challenges and problems in this ﬁeld and endeavor to provide possible solutions.</dcterms:abstract>
        <dc:date>6/2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s11633-018-1126-y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-09-05 07:36:04</dcterms:dateSubmitted>
        <bib:pages>249-266</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1476-8186,%201751-8520">
        <prism:volume>15</prism:volume>
        <dc:title>International Journal of Automation and Computing</dc:title>
        <dc:identifier>DOI 10.1007/s11633-018-1126-y</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Int. J. Autom. Comput.</dcterms:alternative>
        <dc:identifier>ISSN 1476-8186, 1751-8520</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5642">
        <z:itemType>attachment</z:itemType>
        <dc:title>Xue et al_2018_A Survey of Scene Understanding by Event Reasoning in Autonomous Driving.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_5643">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Behley</foaf:surname>
                        <foaf:givenName>Jens</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garbade</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Milioto</foaf:surname>
                        <foaf:givenName>Andres</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Quenzel</foaf:surname>
                        <foaf:givenName>Jan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Behnke</foaf:surname>
                        <foaf:givenName>Sven</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stachniss</foaf:surname>
                        <foaf:givenName>Cyrill</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gall</foaf:surname>
                        <foaf:givenName>Jurgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5644"/>
        <dc:title>SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</dc:title>
        <dc:date>2019-10</dc:date>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5644">
        <z:itemType>attachment</z:itemType>
        <dc:title>Behley et al. - 2019 - SemanticKITTI A Dataset for Semantic Scene Unders.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2302.02790">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bogdoll</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uhlemeyer</foaf:surname>
                        <foaf:givenName>Svenja</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kowol</foaf:surname>
                        <foaf:givenName>Kamil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zöllner</foaf:surname>
                        <foaf:givenName>J. Marius</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5646"/>
        <link:link rdf:resource="#item_5647"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Perception Datasets for Anomaly Detection in Autonomous Driving: A Survey</dc:title>
        <dcterms:abstract>Deep neural networks (DNN) which are employed in perception systems for autonomous driving require a huge amount of data to train on, as they must reliably achieve high performance in all kinds of situations. However, these DNN are usually restricted to a closed set of semantic classes available in their training data, and are therefore unreliable when confronted with previously unseen instances. Thus, multiple perception datasets have been created for the evaluation of anomaly detection methods, which can be categorized into three groups: real anomalies in real-world, synthetic anomalies augmented into real-world and completely synthetic scenes. This survey provides a structured and, to the best of our knowledge, complete overview and comparison of perception datasets for anomaly detection in autonomous driving. Each chapter provides information about tasks and ground truth, context information, and licenses. Additionally, we discuss current weaknesses and gaps in existing datasets to underline the importance of developing further data.</dcterms:abstract>
        <dc:date>2023-02-06</dc:date>
        <z:shortTitle>Perception Datasets for Anomaly Detection in Autonomous Driving</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2302.02790</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-27 15:21:38</dcterms:dateSubmitted>
        <dc:description>arXiv:2302.02790 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2302.02790</dc:identifier>
        <prism:number>arXiv:2302.02790</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5646">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2302.02790</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-27 15:21:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5647">
        <z:itemType>attachment</z:itemType>
        <dc:title>Bogdoll et al_2023_Perception Datasets for Anomaly Detection in Autonomous Driving.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2302.02790.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-27 15:21:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/1&quot;&gt;I Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/2&quot;&gt;II Datasets&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/2&quot;&gt;II-A Lost and Found&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/2&quot;&gt;II-A1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/2&quot;&gt;II-A2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-A3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-B Fishyscapes&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-B1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-B2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-B3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-C CAOS&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-C1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-C2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-C3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-D WD-Pascal&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/3&quot;&gt;II-D1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-D2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-D3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-E Vistas-NP&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-E1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-E2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-E3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-F SegmentMeIfYouCan&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-F1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-F2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-F3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-G CODA&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/4&quot;&gt;II-G1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;II-G2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;II-G3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;II-H Wuppertal OOD Tracking&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;II-H1 Tasks and Ground truth&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;II-H2 Context&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;II-H3 License&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/5&quot;&gt;III Discussion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_FSB75SVV/7&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="#item_5648">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caesar</foaf:surname>
                        <foaf:givenName>Holger</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bankiti</foaf:surname>
                        <foaf:givenName>Varun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lang</foaf:surname>
                        <foaf:givenName>Alex H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vora</foaf:surname>
                        <foaf:givenName>Sourabh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liong</foaf:surname>
                        <foaf:givenName>Venice Erin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Qiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krishnan</foaf:surname>
                        <foaf:givenName>Anush</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baldan</foaf:surname>
                        <foaf:givenName>Giancarlo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beijbom</foaf:surname>
                        <foaf:givenName>Oscar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>nuScenes: A Multimodal Dataset for Autonomous Driving</dc:title>
        <dc:date>2020-06</dc:date>
    </rdf:Description>
    <rdf:Description rdf:about="http://arxiv.org/abs/2004.06320">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Geyer</foaf:surname>
                        <foaf:givenName>Jakob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kassahun</foaf:surname>
                        <foaf:givenName>Yohannes</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mahmudi</foaf:surname>
                        <foaf:givenName>Mentar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ricou</foaf:surname>
                        <foaf:givenName>Xavier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Durgesh</foaf:surname>
                        <foaf:givenName>Rupesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chung</foaf:surname>
                        <foaf:givenName>Andrew S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hauswald</foaf:surname>
                        <foaf:givenName>Lorenz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pham</foaf:surname>
                        <foaf:givenName>Viet Hoang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mühlegg</foaf:surname>
                        <foaf:givenName>Maximilian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dorn</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fernandez</foaf:surname>
                        <foaf:givenName>Tiffany</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jänicke</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mirashi</foaf:surname>
                        <foaf:givenName>Sudesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Savani</foaf:surname>
                        <foaf:givenName>Chiragkumar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sturm</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vorobiov</foaf:surname>
                        <foaf:givenName>Oleksandr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oelker</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garreis</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schuberth</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5650"/>
        <link:link rdf:resource="#item_5651"/>
        <link:link rdf:resource="#item_5652"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Electrical Engineering and Systems Science - Image and Video Processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>A2D2: Audi Autonomous Driving Dataset</dc:title>
        <dcterms:abstract>Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.</dcterms:abstract>
        <dc:date>2020-04-14</dc:date>
        <z:shortTitle>A2D2</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.06320</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-23 12:40:11</dcterms:dateSubmitted>
        <dc:description>arXiv:2004.06320 [cs, eess]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2004.06320</dc:identifier>
        <prism:number>arXiv:2004.06320</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5650">
       <rdf:value>Comment: https://www.a2d2.audi/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5651">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2004.06320</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-23 12:40:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5652">
        <z:itemType>attachment</z:itemType>
        <dc:title>Geyer et al_2020_A2D2.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2004.06320.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-23 12:40:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1912.04838">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Pei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kretzschmar</foaf:surname>
                        <foaf:givenName>Henrik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dotiwalla</foaf:surname>
                        <foaf:givenName>Xerxes</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chouard</foaf:surname>
                        <foaf:givenName>Aurelien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Patnaik</foaf:surname>
                        <foaf:givenName>Vijaysai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tsui</foaf:surname>
                        <foaf:givenName>Paul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>James</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Yin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chai</foaf:surname>
                        <foaf:givenName>Yuning</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caine</foaf:surname>
                        <foaf:givenName>Benjamin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vasudevan</foaf:surname>
                        <foaf:givenName>Vijay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ngiam</foaf:surname>
                        <foaf:givenName>Jiquan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Hang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Timofeev</foaf:surname>
                        <foaf:givenName>Aleksei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ettinger</foaf:surname>
                        <foaf:givenName>Scott</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krivokon</foaf:surname>
                        <foaf:givenName>Maxim</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Amy</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joshi</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Shuyang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shlens</foaf:surname>
                        <foaf:givenName>Jonathon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Zhifeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anguelov</foaf:surname>
                        <foaf:givenName>Dragomir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5654"/>
        <link:link rdf:resource="#item_5655"/>
        <link:link rdf:resource="#item_5656"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Scalability in Perception for Autonomous Driving: Waymo Open Dataset</dc:title>
        <dcterms:abstract>The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.</dcterms:abstract>
        <dc:date>2020-05-12</dc:date>
        <z:shortTitle>Scalability in Perception for Autonomous Driving</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1912.04838</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31 13:29:57</dcterms:dateSubmitted>
        <dc:description>arXiv:1912.04838 [cs, stat]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1912.04838</dc:identifier>
        <prism:number>arXiv:1912.04838</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5654">
       <rdf:value>Comment: CVPR 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5655">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1912.04838</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31 13:30:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5656">
        <z:itemType>attachment</z:itemType>
        <dc:title>Sun et al_2020_Scalability in Perception for Autonomous Driving.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1912.04838.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31 13:30:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2304.01168">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Tianqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Sukmin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ji</foaf:surname>
                        <foaf:givenName>Wenxuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Enze</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ge</foaf:surname>
                        <foaf:givenName>Chongjian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Junsong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Zhenguo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Ping</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5658"/>
        <dc:title>DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving</dc:title>
        <dcterms:abstract>Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset includes 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and prediction tasks. Finally, we present a baseline V2X model named V2XFormer that demonstrates superior performance for motion and accident prediction and 3D object detection compared to the single-vehicle model.</dcterms:abstract>
        <dc:date>2023-08-17</dc:date>
        <z:shortTitle>DeepAccident</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2304.01168</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-11-16 09:35:43</dcterms:dateSubmitted>
        <dc:description>arXiv:2304.01168 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2304.01168</dc:identifier>
        <prism:number>arXiv:2304.01168</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5658">
        <z:itemType>attachment</z:itemType>
        <dc:title>Wang et al_2023_DeepAccident - A Motion and Accident Prediction Benchmark for V2X Autonomous Driving.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2304.01168.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-11-16 09:35:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2112.12610">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Pengchuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shao</foaf:surname>
                        <foaf:givenName>Zhenlei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hao</foaf:surname>
                        <foaf:givenName>Steven</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Zishuo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chai</foaf:surname>
                        <foaf:givenName>Xiaolin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiao</foaf:surname>
                        <foaf:givenName>Judy</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Zesong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Kai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Kun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yunlong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Diange</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5660"/>
        <link:link rdf:resource="#item_5661"/>
        <link:link rdf:resource="#item_5662"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving</dc:title>
        <dcterms:abstract>The accelerating development of autonomous driving technology has placed greater demands on obtaining large amounts of high-quality data. Representative, labeled, real world data serves as the fuel for training deep learning networks, critical for improving self-driving perception algorithms. In this paper, we introduce PandaSet, the first dataset produced by a complete, high-precision autonomous vehicle sensor kit with a no-cost commercial license. The dataset was collected using one 360{\deg} mechanical spinning LiDAR, one forward-facing, long-range LiDAR, and 6 cameras. The dataset contains more than 100 scenes, each of which is 8 seconds long, and provides 28 types of labels for object classification and 37 types of labels for semantic segmentation. We provide baselines for LiDAR-only 3D object detection, LiDAR-camera fusion 3D object detection and LiDAR point cloud segmentation. For more details about PandaSet and the development kit, see https://scale.com/open-datasets/pandaset.</dcterms:abstract>
        <dc:date>2021-12-23</dc:date>
        <z:shortTitle>PandaSet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2112.12610</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-23 21:34:37</dcterms:dateSubmitted>
        <dc:description>arXiv:2112.12610 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2112.12610</dc:identifier>
        <prism:number>arXiv:2112.12610</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5660">
        <rdf:value>Comment: This paper has been published on ITSC'2021, please check the website of the PandaSet for more information: https://pandaset.org/</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5661">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2112.12610</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-23 21:34:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5662">
        <z:itemType>attachment</z:itemType>
        <dc:title>Xiao et al_2021_PandaSet.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2112.12610.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-23 21:34:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/1&quot;&gt;I INTRODUCTION&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/1&quot;&gt;II RELATED WORK&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/2&quot;&gt;III PANDASET DATASET&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/2&quot;&gt;III-A Data Collection&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/3&quot;&gt;III-B Sensor Calibration&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/4&quot;&gt;III-C Scenes Selection&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/4&quot;&gt;III-D Data Annotation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/4&quot;&gt;III-E Dataset Statistics&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/5&quot;&gt;IV BASELINE EXPERIMENTS&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/5&quot;&gt;IV-A LiDAR-only 3D object detection&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/6&quot;&gt;IV-B LiDAR-camera fusion 3D object detection&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/6&quot;&gt;IV-C LiDAR point cloud segmentation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/6&quot;&gt;V CONCLUSION&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_9MIA5ZFX/7&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="#item_5663">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2169-3536"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Qiaoning</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>X. Jessie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Robert</foaf:surname>
                        <foaf:givenName>Lionel P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5664"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Automation</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>explanation</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Bibliographies</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Visualization</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Roads</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Automated vehicle</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>interaction</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Task analysis</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Timing</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>transportation</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Vehicles</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>What and When to Explain? A Survey of the Impact of Explanation on Attitudes Toward Adopting Automated Vehicles</dc:title>
        <dcterms:abstract>Automated vehicles (AV) have the potential to decrease driving-related accidents and traffic congestion and to reduce fuel consumption and carbon emissions. However, because of a lack of trust and acceptance, their widespread adoption is far from certain. One approach researchers have taken to promote trust and acceptance of AVs is to decrease the uncertainty associated with their actions by providing explanations. AV explanations are the reasons the AV provides to make its actions easier to understand. There is now a nascent but rapidly growing body of research on AV explanations. Yet, answers to basic questions like whether or when AV explanations are effective still elude us. To better understand what has been done and what should be done with regard to AV explanations, we present a review of the literature, discuss the findings and identify several important future research directions.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:shortTitle>What and When to Explain?</z:shortTitle>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <bib:pages>159533-159540</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2169-3536">
        <prism:volume>9</prism:volume>
        <dc:title>IEEE Access</dc:title>
        <dc:identifier>DOI 10.1109/ACCESS.2021.3130489</dc:identifier>
        <dc:identifier>ISSN 2169-3536</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5664">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zhang et al. 2021 (published).pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_5665">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yiran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xiaoyin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gong</foaf:surname>
                        <foaf:givenName>Lihang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Hsuan-Chu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Tz-Ying</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yunsheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vasconcelos</foaf:surname>
                        <foaf:givenName>Nuno</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5666"/>
        <dc:title>Explainable Object-Induced Action Decision for Autonomous Vehicles</dc:title>
        <dc:date>2020-06</dc:date>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5666">
        <z:itemType>attachment</z:itemType>
        <dc:title>Xu et al. - 2020 - Explainable Object-Induced Action Decision for Aut.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-5971-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>CHI EA '19</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-5971-9</dc:identifier>
                <dc:title>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</dc:title>
                <dc:identifier>DOI 10.1145/3290607.3312817</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wiegand</foaf:surname>
                        <foaf:givenName>Gesa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmidmaier</foaf:surname>
                        <foaf:givenName>Matthias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weber</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Yuanting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hussmann</foaf:surname>
                        <foaf:givenName>Heinrich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5668"/>
        <dc:subject>autonomous driving</dc:subject>
        <dc:subject>explainability</dc:subject>
        <dc:subject>mental model</dc:subject>
        <dc:subject>situation awareness</dc:subject>
        <dc:title>I Drive - You Trust: Explaining Driving Behavior Of Autonomous Cars</dc:title>
        <dcterms:abstract>Driving in autonomous cars requires trust, especially in case of unexpected driving behavior of the vehicle. This work evaluates mental models that experts and non-expert users have of autonomous driving to provide an explanation of the vehicle's past driving behavior. We identified a target mental model that enhances the user's mental model by adding key components from the mental model experts have. To construct this target mental model and to evaluate a prototype of an explanation visualization we conducted interviews (N=8) and a user study (N=16). The explanation consists of abstract visualizations of different elements, representing the autonomous system's components. We explore the relevance of the explanation's individual elements and their influence on the user's situation awareness. The results show that displaying the detected objects and their predicted motion was most important to understand a situation. After seeing the explanation, the user's level of situation awareness increased significantly.</dcterms:abstract>
        <dc:date>2019</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1145/3290607.3312817</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>event-place: Glasgow, Scotland Uk</dc:description>
        <bib:pages>1–6</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5668">
        <z:itemType>attachment</z:itemType>
        <dc:title>Wiegand et al. - 2019 - I Drive - You Trust Explaining Driving Behavior O.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-7516-0">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>MobileHCI '20</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-7516-0</dc:identifier>
                <dc:title>22nd International Conference on Human-Computer Interaction with Mobile Devices and Services</dc:title>
                <dc:identifier>DOI 10.1145/3379503.3403554</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wiegand</foaf:surname>
                        <foaf:givenName>Gesa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Eiband</foaf:surname>
                        <foaf:givenName>Malin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haubelt</foaf:surname>
                        <foaf:givenName>Maximilian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hussmann</foaf:surname>
                        <foaf:givenName>Heinrich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5675"/>
        <dc:subject>Autonomous driving.</dc:subject>
        <dc:subject>Explainable AI</dc:subject>
        <dc:subject>Unexpected driving behavior</dc:subject>
        <dc:title>“I’d like an Explanation for That!”Exploring Reactions to Unexpected Autonomous Driving</dc:title>
        <dcterms:abstract>Autonomous vehicles are complex systems that may behave in unexpected ways. From the drivers’ perspective, this can cause stress and lower trust and acceptance of autonomous driving. Prior work has shown that explanation of system behavior can mitigate these negative effects. Nevertheless, it remains unclear in which situations drivers actually need an explanation and what kind of interaction is relevant to them. Using thematic analysis of real-world experience reports, we first identified 17 situations in which a vehicle behaved unexpectedly. We then conducted a think-aloud study (N = 26) in a driving simulator to validate these situations and enrich them with qualitative insights about drivers’ need for explanation. We identified six categories to describe the main concerns and topics during unexpected driving behavior (emotion and evaluation, interpretation and reason, vehicle capability, interaction, future driving prediction and explanation request times). Based on these categories, we suggest design implications for autonomous vehicles, in particular related to collaboration insights, user mental models and explanation requests.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1145/3379503.3403554</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>event-place: Oldenburg, Germany</dc:description>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5675">
        <z:itemType>attachment</z:itemType>
        <dc:title>Wiegand et al. - 2020 - “I’d like an Explanation for That!”Exploring React.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-8096-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>CHI '21</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-8096-6</dc:identifier>
                <dc:title>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</dc:title>
                <dc:identifier>DOI 10.1145/3411764.3446647</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schneider</foaf:surname>
                        <foaf:givenName>Tobias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hois</foaf:surname>
                        <foaf:givenName>Joana</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosenstein</foaf:surname>
                        <foaf:givenName>Alischa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghellal</foaf:surname>
                        <foaf:givenName>Sabiha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Theofanou-Fülbier</foaf:surname>
                        <foaf:givenName>Dimitra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gerlicher</foaf:surname>
                        <foaf:givenName>Ansgar R.S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5683"/>
        <dc:subject>autonomous driving</dc:subject>
        <dc:subject>explainable artificial intelligence</dc:subject>
        <dc:subject>user experience</dc:subject>
        <dc:title>ExplAIn Yourself! Transparency for Positive UX in Autonomous Driving</dc:title>
        <dcterms:abstract>In a fully autonomous driving situation, passengers hand over the steering control to a highly automated system. Autonomous driving behaviour may lead to confusion and negative user experience. When establishing such new technology, the user’s acceptance and understanding are crucial factors regarding success and failure. Using a driving simulator and a mobile application, we evaluated if system transparency during and after the interaction can increase the user experience and subjective feeling of safety and control. We contribute an initial guideline for autonomous driving experience design, bringing together the areas of user experience, explainable artificial intelligence and autonomous driving. The AVAM questionnaire, UEQ-S and interviews show that explanations during or after the ride help turn a negative user experience into a neutral one, which might be due to the increased feeling of control. However, we did not detect an effect for combining explanations during and after the ride.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1145/3411764.3446647</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dc:description>event-place: Yokohama, Japan</dc:description>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5683">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/3411764.3446647</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-05-10 06:13:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_5684">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>23</prism:volume>
                <dc:title>IEEE Transactions on Intelligent Transportation Systems</dc:title>
                <dc:identifier>DOI 10.1109/TITS.2021.3122865</dc:identifier>
                <prism:number>8</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Omeiza</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Webb</foaf:surname>
                        <foaf:givenName>Helena</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jirotka</foaf:surname>
                        <foaf:givenName>Marina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kunze</foaf:surname>
                        <foaf:givenName>Lars</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5685"/>
        <dc:title>Explanations in Autonomous Driving: A Survey</dc:title>
        <dc:date>2022</dc:date>
        <bib:pages>10142-10162</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_5685">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:90eab5c0-de54-4f71-b494-908a5c1e5c20/files/ssn009z60m</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-05-10 05:38:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_5704">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2021 IEEE Intelligent Vehicles Symposium (IV)</dc:title>
                <dc:identifier>DOI 10.1109/IV48863.2021.9575917</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Omeiza</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Web</foaf:surname>
                        <foaf:givenName>Helena</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jirotka</foaf:surname>
                        <foaf:givenName>Marina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kunze</foaf:surname>
                        <foaf:givenName>Lars</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5705"/>
        <dc:title>Towards Accountability: Providing Intelligible Explanations in Autonomous Driving</dc:title>
        <dc:date>2021</dc:date>
        <bib:pages>231-237</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5705">
        <z:itemType>attachment</z:itemType>
        <dc:title>Submitted Version</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:dbaa1276-1083-4b64-93c1-8c4635c89d2b/download_file?safe_filename=Omeiza_et_al_2021_towards_accountability_providing.pdf&amp;file_format=pdf&amp;type_of_work=Conference+item</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-05-10 05:54:43</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_5714">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>161</prism:volume>
                <dc:title>Technological Forecasting and Social Change</dc:title>
                <dc:identifier>DOI 10.1016/j.techfore.2020.120319</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nastjuk</foaf:surname>
                        <foaf:givenName>Ilja</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Herrenkind</foaf:surname>
                        <foaf:givenName>Bernd</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marrone</foaf:surname>
                        <foaf:givenName>Mauricio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brendel</foaf:surname>
                        <foaf:givenName>Alfred</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kolbe</foaf:surname>
                        <foaf:givenName>Lutz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5715"/>
        <dc:title>What drives the acceptance of autonomous driving? An investigation of acceptance factors from an end-user's perspective</dc:title>
        <dc:date>2020-10</dc:date>
    </bib:Article>
    <z:Attachment rdf:about="#item_5715">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0040162520311458/pdfft?md5=9acbf0183f3219de88e3d601c6aa0693&amp;pid=1-s2.0-S0040162520311458-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-05-18 06:35:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1809.04843">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Codevilla</foaf:surname>
                        <foaf:givenName>Felipe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>López</foaf:surname>
                        <foaf:givenName>Antonio M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koltun</foaf:surname>
                        <foaf:givenName>Vladlen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dosovitskiy</foaf:surname>
                        <foaf:givenName>Alexey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5717"/>
        <link:link rdf:resource="#item_5718"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>On Offline Evaluation of Vision-based Driving Models</dc:title>
        <dcterms:abstract>Autonomous driving models should ideally be evaluated by deploying them on a ﬂeet of physical vehicles in the real world. Unfortunately, this approach is not practical for the vast majority of researchers. An attractive alternative is to evaluate models ofﬂine, on a pre-collected validation dataset with ground truth annotation. In this paper, we investigate the relation between various online and ofﬂine metrics for evaluation of autonomous driving models. We ﬁnd that ofﬂine prediction error is not necessarily correlated with driving quality, and two models with identical prediction error can differ dramatically in their driving performance. We show that the correlation of ofﬂine evaluation with driving quality can be signiﬁcantly improved by selecting an appropriate validation dataset and suitable ofﬂine metrics.</dcterms:abstract>
        <dc:date>2018-09-13</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1809.04843</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31 13:36:26</dcterms:dateSubmitted>
        <dc:description>arXiv:1809.04843 [cs]</dc:description>
        <prism:number>arXiv:1809.04843</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5717">
       <rdf:value>Comment: Published at the ECCV 2018 conference</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5718">
        <z:itemType>attachment</z:itemType>
        <dc:title>Codevilla et al_2018_On Offline Evaluation of Vision-based Driving Models.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1809.04843.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31 13:36:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://doi.org/10.1109/ITSC55140.2022.9921776">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</dc:title>
                <dc:identifier>DOI 10.1109/ITSC55140.2022.9921776</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Macau, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ramakrishna</foaf:surname>
                        <foaf:givenName>Shreyas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Baiting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kuhn</foaf:surname>
                        <foaf:givenName>Christopher B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karsai</foaf:surname>
                        <foaf:givenName>Gabor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dubey</foaf:surname>
                        <foaf:givenName>Abhishek</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5720"/>
        <dc:title>ANTI-CARLA: An Adversarial Testing Framework for Autonomous Vehicles in CARLA</dc:title>
        <dcterms:abstract>Despite recent advances in autonomous driving systems, accidents such as the fatal Uber crash in 2018 show these systems are still susceptible to edge cases. Such systems must be thoroughly tested and validated before being deployed in the real world to avoid such events. Testing in open-world scenarios can be difficult, time-consuming, and expensive. These challenges can be addressed by using driving simulators such as CARLA instead. A key part of such tests is adversarial testing, in which the goal is to find scenarios that lead to failures of the given system. While several independent efforts in testing have been made, a well-established testing framework that enables adversarial testing has yet to be made available for CARLA. We therefore propose ANTI-CARLA, an automated testing framework in CARLA for simulating adversarial weather conditions (e.g., heavy rain) and sensor faults (e.g., camera occlusion) that fail the system. The operating conditions in which a given system should be tested are specified in a scenario description language. The framework offers an efficient search mechanism that searches for adversarial operating conditions that will fail the tested system. In this way, ANTI-CARLA extends the CARLA simulator with the capability of performing adversarial testing on any given driving pipeline. We use ANTI-CARLA to test the driving pipeline trained with Learning By Cheating (LBC) approach. The simulation results demonstrate that ANTI-CARLA can effectively and automatically find a range of failure cases despite LBC reaching an accuracy of 100&amp;#x0025; in the CARLA benchmark.</dcterms:abstract>
        <dc:date>Oktober 8, 2022</dc:date>
        <z:shortTitle>ANTI-CARLA</z:shortTitle>
        <z:libraryCatalog>ACM Digital Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://doi.org/10.1109/ITSC55140.2022.9921776</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31</dcterms:dateSubmitted>
        <bib:pages>2620–2627</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5720">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ramakrishna et al_2022_ANTI-CARLA.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2208.06309</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-31 13:32:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/1&quot;&gt;I Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/2&quot;&gt;II Related Work&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/2&quot;&gt;II-A Test Case Description and Sampling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/2&quot;&gt;II-B Testing Frameworks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/2&quot;&gt;II-C Autonomous Driving Pipelines&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/3&quot;&gt;III Problem Formulation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/3&quot;&gt;IV ANTI-CARLA Framework&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/3&quot;&gt;IV-A Scenario Generator&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/4&quot;&gt;IV-B Adapter Glue Code&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/5&quot;&gt;IV-C Scoring Function&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/5&quot;&gt;IV-D Samplers&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/5&quot;&gt;V Evaluation&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/5&quot;&gt;V-A Simulation Setup&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/6&quot;&gt;V-A.1 Comparison Metrics&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/6&quot;&gt;V-B Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/6&quot;&gt;V-B.1 Visualization&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/6&quot;&gt;V-B.2 Sampler Comparison&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/7&quot;&gt;V-B.3 Town-wise Infractions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/7&quot;&gt;V-B.4 Controller Comparison&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/8&quot;&gt;V-B.5 Recommendations&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/8&quot;&gt;VI Conclusion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_Y7RHDAV5/8&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="https://openreview.net/forum?id=ZjN2AuXgu1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Shengjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mi</foaf:surname>
                        <foaf:givenName>Lan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gilpin</foaf:surname>
                        <foaf:givenName>Leilani H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>A Framework for Generating Dangerous Scenes for Testing Robustness</dc:title>
        <dcterms:abstract>Benchmark datasets for autonomous driving, such as KITTI, nuScenes, Argoverse, or Waymo are realistic but designed to be faultless. These datasets do not contain errors, difficult driving maneuvers, or other corner cases. We propose a framework for perturbing autonomous vehicle datasets, the DANGER framework, which generates edge-case images on top of current autonomous driving datasets. The input to DANGER are photorealistic datasets from real driving scenarios. We present the DANGER algorithm for vehicle position manipulation and the interface towards the renderer module, and present five scenario-level dangerous primitives generation applied to the virtual KITTI and virtual KITTI 2 datasets. Our experiments prove that DANGER can be used as a framework for expanding the current datasets to cover generative while realistic and anomalous corner cases.</dcterms:abstract>
        <dc:date>2022/11/23</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>openreview.net</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://openreview.net/forum?id=ZjN2AuXgu1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-03-27 15:15:36</dcterms:dateSubmitted>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Progress and Challenges in Building Trustworthy Embodied AI</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <rdf:Description rdf:about="http://arxiv.org/abs/1910.03088">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhan</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Liting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Di</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>Haojie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clausse</foaf:surname>
                        <foaf:givenName>Aubrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Naumann</foaf:surname>
                        <foaf:givenName>Maximilian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kummerle</foaf:surname>
                        <foaf:givenName>Julius</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Konigshof</foaf:surname>
                        <foaf:givenName>Hendrik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stiller</foaf:surname>
                        <foaf:givenName>Christoph</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de La Fortelle</foaf:surname>
                        <foaf:givenName>Arnaud</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tomizuka</foaf:surname>
                        <foaf:givenName>Masayoshi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5801"/>
        <dc:title>INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps</dc:title>
        <dcterms:abstract>Behavior-related research areas such as motion prediction/planning, representation/imitation learning, behavior modeling/generation, and algorithm testing, require support from high-quality motion datasets containing interactive driving scenarios with different driving cultures. In this paper, we present an INTERnational, Adversarial and Cooperative moTION dataset (INTERACTION dataset) in interactive driving scenarios with semantic maps. Five features of the dataset are highlighted. 1) The interactive driving scenarios are diverse, including urban/highway/ramp merging and lane changes, roundabouts with yield/stop signs, signalized intersections, intersections with one/two/all-way stops, etc. 2) Motion data from different countries and different continents are collected so that driving preferences and styles in different cultures are naturally included. 3) The driving behavior is highly interactive and complex with adversarial and cooperative motions of various traffic participants. Highly complex behavior such as negotiations, aggressive/irrational decisions and traffic rule violations are densely contained in the dataset, while regular behavior can also be found from cautious car-following, stop, left/right/U-turn to rational lane-change and cycling and pedestrian crossing, etc. 4) The levels of criticality span wide, from regular safe operations to dangerous, near-collision maneuvers. Real collision, although relatively slight, is also included. 5) Maps with complete semantic information are provided with physical layers, reference lines, lanelet connections and traffic rules. The data is recorded from drones and traffic cameras. Statistics of the dataset in terms of number of entities and interaction density are also provided, along with some utilization examples in a variety of behavior-related research areas. The dataset can be downloaded via https://interaction-dataset.com.</dcterms:abstract>
        <dc:date>2019-09-30</dc:date>
        <z:shortTitle>INTERACTION Dataset</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1910.03088</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-05 10:27:12</dcterms:dateSubmitted>
        <dc:description>arXiv:1910.03088 [cs, eess]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1910.03088</dc:identifier>
        <prism:number>arXiv:1910.03088</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5801">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zhan et al_2019_INTERACTION Dataset - An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1910.03088.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-05 10:27:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/1&quot;&gt;I Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/2&quot;&gt;II Related Work&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/2&quot;&gt;II-A Datasets from Bird's Eye View&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/3&quot;&gt;II-B Datasets from Onboard Sensors&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/3&quot;&gt;III Features of the Dataset&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/3&quot;&gt;III-A Diversity&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/4&quot;&gt;III-B Internationality&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/5&quot;&gt;III-C Complexity&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/5&quot;&gt;III-D Criticality&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/6&quot;&gt;III-E Semantic Map&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/6&quot;&gt;IV Construction of Motion Data and Maps&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/6&quot;&gt;IV-A Motions from Drone Data&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/6&quot;&gt;IV-B Motions from Traffic Camera Data&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/6&quot;&gt;IV-C Construction of the High Definition Maps&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/7&quot;&gt;V Statistics of the Dataset&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/7&quot;&gt;V-A Scenarios and Vehicle Density&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/7&quot;&gt;V-B Metrics for Interactive Behavior Identification&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/7&quot;&gt;V-C Distribution of Interactivity&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/8&quot;&gt;VI Utilization Examples&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/8&quot;&gt;VI-A Motion Prediction and Behavior Analysis&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/8&quot;&gt;VI-B Imitation Learning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/8&quot;&gt;VI-C Validation of Decision and Planning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/10&quot;&gt;VI-D Motion Clustering and Representation Learning&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/11&quot;&gt;VI-E Extraction of Interactive Agents and Trajectories.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/11&quot;&gt;VI-F Human-like Decision and Behavior Generation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/11&quot;&gt;VII Conclusion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/11&quot;&gt;VIII Acknowledgement&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_PUERTMIA/11&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2307.13924">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ivanovic</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Guanyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gilitschenski</foaf:surname>
                        <foaf:givenName>Igor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pavone</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5802"/>
        <link:link rdf:resource="#item_5800"/>
        <dc:title>trajdata: A Unified Interface to Multiple Human Trajectory Datasets</dc:title>
        <dcterms:abstract>The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata</dcterms:abstract>
        <dc:date>2023-07-25</dc:date>
        <z:shortTitle>trajdata</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2307.13924</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-05 10:26:59</dcterms:dateSubmitted>
        <dc:description>arXiv:2307.13924 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2307.13924</dc:identifier>
        <prism:number>arXiv:2307.13924</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5802">
       <rdf:value>Comment: 15 pages, 15 figures, 3 tables</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5800">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ivanovic et al_2023_trajdata - A Unified Interface to Multiple Human Trajectory Datasets.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2307.13924.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-02-05 10:27:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/1&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/2&quot;&gt;Related Work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/2&quot;&gt;trajdata: A Unified Interface to Multiple Human Trajectory Datasets&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/2&quot;&gt;Standardized Trajectory and Map Formats&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/3&quot;&gt;Core trajdata Functionalities&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/4&quot;&gt;Dataset Comparisons and Analyses&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/5&quot;&gt;Agent Distributions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/6&quot;&gt;Motion Complexity&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/7&quot;&gt;Annotation Quality&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/8&quot;&gt;Conclusions and Recommendations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/13&quot;&gt;Additional Dataset Details&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/13&quot;&gt;Map Statistics&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/13&quot;&gt;Stationary Agents&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/13&quot;&gt;Acceleration and Jerk&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/14&quot;&gt;Heading Distributions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/3_ZH6C7377/14&quot;&gt;Path Efficiency&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1807.11546">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of the European Conference on Computer Vision (ECCV)</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Jinkyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rohrbach</foaf:surname>
                        <foaf:givenName>Anna</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darrell</foaf:surname>
                        <foaf:givenName>Trevor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Canny</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akata</foaf:surname>
                        <foaf:givenName>Zeynep</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5843"/>
        <link:link rdf:resource="#item_5842"/>
        <link:link rdf:resource="#item_5841"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Textual Explanations for Self-Driving Vehicles</dc:title>
        <dcterms:abstract>Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller's output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e., acceleration and change of course. The controller's attention identifies image regions that potentially influence the network's output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.</dcterms:abstract>
        <dc:date>2018-07-30</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1807.11546</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-08 08:42:31</dcterms:dateSubmitted>
        <dc:description>arXiv:1807.11546 [cs]</dc:description>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5843">
       <rdf:value>Comment: Accepted to ECCV 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5842">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1807.11546.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-08 08:42:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5841">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1807.11546</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-08 08:42:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://arxiv.org/abs/2208.00249">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI 10.48550/ARXIV.2208.00249</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ayoub</foaf:surname>
                        <foaf:givenName>Jackie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Zifei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Meitang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Huizhong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sherony</foaf:surname>
                        <foaf:givenName>Rini</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bao</foaf:surname>
                        <foaf:givenName>Shan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computation and Language (cs.CL)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>FOS: Computer and information sciences</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Cause-and-Effect Analysis of ADAS: A Comparison Study between Literature Review and Complaint Data</dc:title>
        <dcterms:abstract>Advanced driver assistance systems (ADAS) are designed to improve vehicle safety. However, it is difficult to achieve such benefits without understanding the causes and limitations of the current ADAS and their possible solutions. This study 1) investigated the limitations and solutions of ADAS through a literature review, 2) identified the causes and effects of ADAS through consumer complaints using natural language processing models, and 3) compared the major differences between the two. These two lines of research identified similar categories of ADAS causes, including human factors, environmental factors, and vehicle factors. However, academic research focused more on human factors of ADAS issues and proposed advanced algorithms to mitigate such issues while drivers complained more of vehicle factors of ADAS failures, which led to associated top consequences. The findings from these two sources tend to complement each other and provide important implications for the improvement of ADAS in the future.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:shortTitle>Cause-and-Effect Analysis of ADAS</z:shortTitle>
        <z:libraryCatalog>DOI.org (Datacite)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2208.00249</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-08 08:41:17</dcterms:dateSubmitted>
        <dc:rights>Creative Commons Attribution 4.0 International</dc:rights>
        <dc:description>Publisher: [object Object]
Version Number: 1</dc:description>
    </bib:Article>
    <bib:Article rdf:about="#item_5840">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>5</prism:volume>
                <dc:title>Trafik Ve Ulaşım Araştırmaları Dergisi / Journal of Transportation Research</dc:title>
                <dc:identifier>DOI https://doi.org/10.38002/tuad.1084567</dc:identifier>
                <prism:number>1</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ergin</foaf:surname>
                        <foaf:givenName>U.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5844"/>
        <dc:title>One of the First Fatalities of a Self-Driving Car: Root Cause Analysis of the 2016 Tesla Model S 70D Crash.</dc:title>
        <dc:date>2022</dc:date>
        <bib:pages>83-97</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_5844">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ergin - 2022 - One of the First Fatalities of a Self-Driving Car.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2205.04712">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wörmann</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bogdoll</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brunner</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bührle</foaf:surname>
                        <foaf:givenName>Etienne</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Han</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chuo</foaf:surname>
                        <foaf:givenName>Evaristus Fuh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cvejoski</foaf:surname>
                        <foaf:givenName>Kostadin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van Elst</foaf:surname>
                        <foaf:givenName>Ludger</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gottschall</foaf:surname>
                        <foaf:givenName>Philip</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Griesche</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hellert</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hesels</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Houben</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joseph</foaf:surname>
                        <foaf:givenName>Tim</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Keil</foaf:surname>
                        <foaf:givenName>Niklas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kelsch</foaf:surname>
                        <foaf:givenName>Johann</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Keser</foaf:surname>
                        <foaf:givenName>Mert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Königshof</foaf:surname>
                        <foaf:givenName>Hendrik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kraft</foaf:surname>
                        <foaf:givenName>Erwin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kreuser</foaf:surname>
                        <foaf:givenName>Leonie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krone</foaf:surname>
                        <foaf:givenName>Kevin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Latka</foaf:surname>
                        <foaf:givenName>Tobias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mattern</foaf:surname>
                        <foaf:givenName>Denny</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Matthes</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Motzkus</foaf:surname>
                        <foaf:givenName>Franz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Munir</foaf:surname>
                        <foaf:givenName>Mohsin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nekolla</foaf:surname>
                        <foaf:givenName>Moritz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paschke</foaf:surname>
                        <foaf:givenName>Adrian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>von Pilchau</foaf:surname>
                        <foaf:givenName>Stefan Pilar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pintz</foaf:surname>
                        <foaf:givenName>Maximilian Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qiu</foaf:surname>
                        <foaf:givenName>Tianming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qureishi</foaf:surname>
                        <foaf:givenName>Faraz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rizvi</foaf:surname>
                        <foaf:givenName>Syed Tahseen Raza</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reichardt</foaf:surname>
                        <foaf:givenName>Jörg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>von Rueden</foaf:surname>
                        <foaf:givenName>Laura</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sagel</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sasdelli</foaf:surname>
                        <foaf:givenName>Diogo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scholl</foaf:surname>
                        <foaf:givenName>Tobias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schunk</foaf:surname>
                        <foaf:givenName>Gerhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schwalbe</foaf:surname>
                        <foaf:givenName>Gesina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shoeb</foaf:surname>
                        <foaf:givenName>Youssef</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stapelbroek</foaf:surname>
                        <foaf:givenName>Hendrik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stehr</foaf:surname>
                        <foaf:givenName>Vera</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srinivas</foaf:surname>
                        <foaf:givenName>Gurucharan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tran</foaf:surname>
                        <foaf:givenName>Anh Tuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vivekanandan</foaf:surname>
                        <foaf:givenName>Abhishek</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Ya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wasserrab</foaf:surname>
                        <foaf:givenName>Florian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Werner</foaf:surname>
                        <foaf:givenName>Tino</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wirth</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zwicklbauer</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5878"/>
        <link:link rdf:resource="#item_5877"/>
        <link:link rdf:resource="#item_5876"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey</dc:title>
        <dcterms:abstract>The availability of representative datasets is an essential prerequisite for many successful artificial intelligence and machine learning models. However, in real life applications these models often encounter scenarios that are inadequately represented in the data used for training. There are various reasons for the absence of sufficient data, ranging from time and cost constraints to ethical considerations. As a consequence, the reliable usage of these models, especially in safety-critical applications, is still a tremendous challenge. Leveraging additional, already existing sources of knowledge is key to overcome the limitations of purely data-driven approaches. Knowledge augmented machine learning approaches offer the possibility of compensating for deficiencies, errors, or ambiguities in the data, thus increasing the generalization capability of the applied models. Even more, predictions that conform with knowledge are crucial for making trustworthy and safe decisions even in underrepresented scenarios. This work provides an overview of existing techniques and methods in the literature that combine data-driven models with existing knowledge. The identified approaches are structured according to the categories knowledge integration, extraction and conformity. In particular, we address the application of the presented methods in the field of autonomous driving.</dcterms:abstract>
        <dc:date>2023-11-20</dc:date>
        <z:shortTitle>Knowledge Augmented Machine Learning with Applications in Autonomous Driving</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2205.04712</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:35:19</dcterms:dateSubmitted>
        <dc:description>arXiv:2205.04712 [cs]</dc:description>
        <prism:number>arXiv:2205.04712</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5878">
        <rdf:value>Comment: 111 pages, Added section on Run-time Network Verification</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5877">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2205.04712.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:35:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5876">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2205.04712</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:35:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2207.07496">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Avetisyan</foaf:surname>
                        <foaf:givenName>Lilit</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ayoub</foaf:surname>
                        <foaf:givenName>Jackie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5880"/>
        <link:link rdf:resource="#item_5879"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Human-Computer Interaction</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Investigating Explanations in Conditional and Highly Automated Driving: The Effects of Situation Awareness and Modality</dc:title>
        <dcterms:abstract>With the level of automation increases in vehicles, such as conditional and highly automated vehicles (AVs), drivers are becoming increasingly out of the control loop, especially in unexpected driving scenarios. Although it might be not necessary to require the drivers to intervene on most occasions, it is still important to improve drivers' situation awareness (SA) in unexpected driving scenarios to improve their trust in and acceptance of AVs. In this study, we conceptualized SA at the levels of perception (SA L1), comprehension (SA L2), and projection (SA L3), and proposed an SA level-based explanation framework based on explainable AI. Then, we examined the effects of these explanations and their modalities on drivers' situational trust, cognitive workload, as well as explanation satisfaction. A three (SA levels: SA L1, SA L2 and SA L3) by two (explanation modalities: visual, visual + audio) between-subjects experiment was conducted with 340 participants recruited from Amazon Mechanical Turk. The results indicated that by designing the explanations using the proposed SA-based framework, participants could redirect their attention to the important objects in the traffic and understand their meaning for the AV system. This improved their SA and filled the gap of understanding the correspondence of AV's behavior in the particular situations which also increased their situational trust in AV. The results showed that participants reported the highest trust with SA L2 explanations, although the mental workload was assessed higher in this level. The results also provided insights into the relationship between the amount of information in explanations and modalities, showing that participants were more satisfied with visual-only explanations in the SA L1 and SA L2 conditions and were more satisfied with visual and auditory explanations in the SA L3 condition.</dcterms:abstract>
        <dc:date>2022-07-15</dc:date>
        <z:shortTitle>Investigating Explanations in Conditional and Highly Automated Driving</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2207.07496</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:33:42</dcterms:dateSubmitted>
        <dc:description>arXiv:2207.07496 [cs]</dc:description>
        <prism:number>arXiv:2207.07496</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5880">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2207.07496.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:33:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5879">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2207.07496</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:34:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://dergipark.org.tr/en/doi/10.38002/tuad.1084567">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2667-8071"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ergin</foaf:surname>
                        <foaf:givenName>Uluğhan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5881"/>
        <dc:title>One of the First Fatalities of a Self-Driving Car: Root Cause Analysis of the 2016 Tesla Model S 70D Crash</dc:title>
        <dcterms:abstract>The recent technological developments have increased the prevalence of automated vehicles and vehicles with Advanced Driver Assistance Systems (ADAS) within the roadway traffic. Consequently, different safety-critical concerns rise for the usage of self-driving vehicles. The present study has investigated a crash between a semi-trailer of a Freightliner Truck and an SAE Level 2 automated Tesla Car. Operated during the autopilot mode engaged, the 40-year-old Tesla Driver hit and traveled under the mid aspect of the semi-trailer without taking any evasive actions prior to the crash and instantly deceased after the initial impact. The contributory factors of Human Error and Equipment Failure have been analyzed using specific tools of the root cause analysis: Five Whys Technique and Barrier Analysis respectively. The analysis has emphasized the importance of situational awareness while driving automated vehicles and showed that safety barrier features of ADAS may fail and should not be over-relied. The potential reasons to over-rely automated systems were discussed, and recommendations that target the safety of automated vehicle drivers have been made.</dcterms:abstract>
        <dc:date>2022-04-30</dc:date>
        <z:shortTitle>One of the First Fatalities of a Self-Driving Car</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://dergipark.org.tr/en/doi/10.38002/tuad.1084567</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:33:23</dcterms:dateSubmitted>
        <bib:pages>83-97</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2667-8071">
        <prism:volume>5</prism:volume>
        <dc:title>Trafik ve Ulaşım Araştırmaları Dergisi</dc:title>
        <dc:identifier>DOI 10.38002/tuad.1084567</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2667-8071</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5881">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dergipark.org.tr/en/download/article-file/2296986</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:33:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_5872">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Jinkyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rohrbach</foaf:surname>
                        <foaf:givenName>Anna</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darrell</foaf:surname>
                        <foaf:givenName>Trevor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Canny</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akata</foaf:surname>
                        <foaf:givenName>Zeynep</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5884"/>
        <link:link rdf:resource="#item_5883"/>
        <link:link rdf:resource="#item_5882"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Textual Explanations for Self-Driving Vehicles</dc:title>
        <dcterms:abstract>Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller's output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e., acceleration and change of course. The controller's attention identifies image regions that potentially influence the network's output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.</dcterms:abstract>
        <dc:date>2018-07-30</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1807.11546</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:53</dcterms:dateSubmitted>
        <dc:description>arXiv:1807.11546 [cs]</dc:description>
        <prism:number>arXiv:1807.11546</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5884">
       <rdf:value>Comment: Accepted to ECCV 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5883">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1807.11546.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5882">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1807.11546</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2312.06352">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Inoue</foaf:surname>
                        <foaf:givenName>Yuichi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yada</foaf:surname>
                        <foaf:givenName>Yuki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tanahashi</foaf:surname>
                        <foaf:givenName>Kotaro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yamaguchi</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5887"/>
        <link:link rdf:resource="#item_5886"/>
        <link:link rdf:resource="#item_5885"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Computation and Language</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations</dc:title>
        <dcterms:abstract>Visual Question Answering (VQA) is one of the most important tasks in autonomous driving, which requires accurate recognition and complex situation evaluations. However, datasets annotated in a QA format, which guarantees precise language generation and scene recognition from driving scenes, have not been established yet. In this work, we introduce Markup-QA, a novel dataset annotation technique in which QAs are enclosed within markups. This approach facilitates the simultaneous evaluation of a model's capabilities in sentence generation and VQA. Moreover, using this annotation methodology, we designed the NuScenes-MQA dataset. This dataset empowers the development of vision language models, especially for autonomous driving tasks, by focusing on both descriptive capabilities and precise QA. The dataset is available at https://github.com/turingmotors/NuScenes-MQA.</dcterms:abstract>
        <dc:date>2023-12-11</dc:date>
        <z:shortTitle>NuScenes-MQA</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2312.06352</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:28</dcterms:dateSubmitted>
        <dc:description>arXiv:2312.06352 [cs]</dc:description>
        <prism:number>arXiv:2312.06352</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5887">
       <rdf:value>Comment: Accepted at LLVM-AD Workshop @ WACV 2024</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5886">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2312.06352.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5885">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2312.06352</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/10268664/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1524-9050,%201558-0016"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maier</foaf:surname>
                        <foaf:givenName>Robert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grabinger</foaf:surname>
                        <foaf:givenName>Lisa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Urlhart</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mottok</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5888"/>
        <dc:title>Causal Models to Support Scenario-Based Testing of ADAS</dc:title>
        <dc:date>2023</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10268664/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:31:59</dcterms:dateSubmitted>
        <dc:rights>https://creativecommons.org/licenses/by/4.0/legalcode</dc:rights>
        <bib:pages>1-17</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1524-9050,%201558-0016">
        <dc:title>IEEE Transactions on Intelligent Transportation Systems</dc:title>
        <dc:identifier>DOI 10.1109/TITS.2023.3317475</dc:identifier>
        <dcterms:alternative>IEEE Trans. Intell. Transport. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 1524-9050, 1558-0016</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5888">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/ielx7/6979/4358928/10268664.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:32:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2210.15375">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koopmann</foaf:surname>
                        <foaf:givenName>Tjark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Neurohr</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Putze</foaf:surname>
                        <foaf:givenName>Lina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Westhofen</foaf:surname>
                        <foaf:givenName>Lukas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gansch</foaf:surname>
                        <foaf:givenName>Roman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Adee</foaf:surname>
                        <foaf:givenName>Ahmad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5890"/>
        <link:link rdf:resource="#item_5889"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Applications</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Grasping Causality for the Explanation of Criticality for Automated Driving</dc:title>
        <dcterms:abstract>The verification and validation of automated driving systems at SAE levels 4 and 5 is a multi-faceted challenge for which classical statistical considerations become infeasible. For this, contemporary approaches suggest a decomposition into scenario classes combined with statistical analysis thereof regarding the emergence of criticality. Unfortunately, these associational approaches may yield spurious inferences, or worse, fail to recognize the causalities leading to critical scenarios, which are, in turn, prerequisite for the development and safeguarding of automated driving systems. As to incorporate causal knowledge within these processes, this work introduces a formalization of causal queries whose answers facilitate a causal understanding of safety-relevant influencing factors for automated driving. This formalized causal knowledge can be used to specify and implement abstract safety principles that provably reduce the criticality associated with these influencing factors. Based on Judea Pearl's causal theory, we define a causal relation as a causal structure together with a context, both related to a domain ontology, where the focus lies on modeling the effect of such influencing factors on criticality as measured by a suitable metric. As to assess modeling quality, we suggest various quantities and evaluate them on a small example. As availability and quality of data are imperative for validly estimating answers to the causal queries, we also discuss requirements on real-world and synthetic data acquisition. We thereby contribute to establishing causal considerations at the heart of the safety processes that are urgently needed as to ensure the safe operation of automated driving systems.</dcterms:abstract>
        <dc:date>2022-10-27</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2210.15375</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:27:21</dcterms:dateSubmitted>
        <dc:description>arXiv:2210.15375 [cs, stat]</dc:description>
        <prism:number>arXiv:2210.15375</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5890">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2210.15375.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:27:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5889">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2210.15375</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-27 08:27:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_5931">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1939-3520"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Xi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Tianyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Huai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lou</foaf:surname>
                        <foaf:givenName>Guannan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Miryung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Tsong Yueh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5932"/>
        <dc:title>A Declarative Metamorphic Testing Framework for Autonomous Driving</dc:title>
        <dcterms:abstract>Autonomous driving has gained much attention from both industry and academia. Currently, Deep Neural Networks (DNNs) are widely used for perception and control in autonomous driving. However, several fatal accidents caused by autonomous vehicles have raised serious safety concerns about autonomous driving models. Some recent studies have successfully used the metamorphic testing technique to detect thousands of potential issues in some popularly used autonomous driving models. However, prior study is limited to a small set of metamorphic relations, which do not reflect rich, real-world traffic scenarios and are also not customizable. This paper presents a novel declarative rule-based metamorphic testing framework called RMT. RMT provides a rule template with natural language syntax, allowing users to flexibly specify an enriched set of testing scenarios based on real-world traffic rules and domain knowledge. RMT automatically parses human-written rules to metamorphic relations using an NLP-based rule parser referring to an ontology list and generates test cases with a variety of image transformation engines. We evaluated RMT on three autonomous driving models. With an enriched set of metamorphic relations, RMT detected a significant number of abnormal model predictions that were not detected by prior work. Through a large-scale human study on Amazon Mechanical Turk, we further confirmed the authenticity of test cases generated by RMT and the validity of detected abnormal model predictions.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:description>Conference Name: IEEE Transactions on Software Engineering</dc:description>
        <bib:pages>1-20</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1939-3520">
        <dc:title>IEEE Transactions on Software Engineering</dc:title>
        <dc:identifier>DOI 10.1109/TSE.2022.3206427</dc:identifier>
        <dc:identifier>ISSN 1939-3520</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5932">
        <z:itemType>attachment</z:itemType>
        <dc:title>Deng et al_2022_A Declarative Metamorphic Testing Framework for Autonomous Driving.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2012.10672</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-31 14:09:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Qingzhao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Shengtuo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jiachen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Qi Alfred</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>Z. Morley</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5952"/>
        <dc:title>On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles</dc:title>
        <dc:date>2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:30:41</dcterms:dateSubmitted>
        <bib:pages>15159-15168</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5952">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zhang et al_2022_On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:30:43</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_TrajPAC_Towards_Robustness_Verification_of_Pedestrian_Trajectory_Prediction_Models_ICCV_2023_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Liang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Nathaniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Pengfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>Gaojie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Cheng-Chao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Lijun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5954"/>
        <dc:title>TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models</dc:title>
        <dc:date>2023</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>TrajPAC</z:shortTitle>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_TrajPAC_Towards_Robustness_Verification_of_Pedestrian_Trajectory_Prediction_Models_ICCV_2023_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-06 19:55:09</dcterms:dateSubmitted>
        <bib:pages>8327-8339</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF International Conference on Computer Vision</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5954">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zhang et al_2023_TrajPAC - Towards Robustness Verification of Pedestrian Trajectory Prediction Models.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_TrajPAC_Towards_Robustness_Verification_of_Pedestrian_Trajectory_Prediction_Models_ICCV_2023_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-06 19:55:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/WACV2023/html/Zheng_Robustness_of_Trajectory_Prediction_Models_Under_Map-Based_Attacks_WACV_2023_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Zhihao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ying</foaf:surname>
                        <foaf:givenName>Xiaowen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>Zhen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chuah</foaf:surname>
                        <foaf:givenName>Mooi Choo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5951"/>
        <dc:title>Robustness of Trajectory Prediction Models Under Map-Based Attacks</dc:title>
        <dc:date>2023</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/WACV2023/html/Zheng_Robustness_of_Trajectory_Prediction_Models_Under_Map-Based_Attacks_WACV_2023_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:32:15</dcterms:dateSubmitted>
        <bib:pages>4541-4550</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5951">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zheng et al_2023_Robustness of Trajectory Prediction Models Under Map-Based Attacks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_Robustness_of_Trajectory_Prediction_Models_Under_Map-Based_Attacks_WACV_2023_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:32:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GroupNet_Multiscale_Hypergraph_Neural_Networks_for_Trajectory_Prediction_With_Relational_CVPR_2022_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Chenxin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Maosen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ni</foaf:surname>
                        <foaf:givenName>Zhenyang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Ya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Siheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5953"/>
        <dc:title>GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction With Relational Reasoning</dc:title>
        <dc:date>2022</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>GroupNet</z:shortTitle>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GroupNet_Multiscale_Hypergraph_Neural_Networks_for_Trajectory_Prediction_With_Relational_CVPR_2022_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:56:12</dcterms:dateSubmitted>
        <bib:pages>6498-6507</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5953">
        <z:itemType>attachment</z:itemType>
        <dc:title>Xu et al_2022_GroupNet - Multiscale Hypergraph Neural Networks for Trajectory Prediction With Relational Reasoning.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GroupNet_Multiscale_Hypergraph_Neural_Networks_for_Trajectory_Prediction_With_Relational_CVPR_2022_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:56:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://proceedings.mlr.press/v211/tan23a.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of The 5th Annual Learning for Dynamics and Control Conference</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>PMLR</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tan</foaf:surname>
                        <foaf:givenName>Kaiyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Jun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kantaros</foaf:surname>
                        <foaf:givenName>Yiannis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5957"/>
        <dc:title>Targeted Adversarial Attacks against Neural Network Trajectory Predictors</dc:title>
        <dcterms:abstract>Trajectory prediction is an integral component of modern autonomous systems as it allows for envisioning future intentions of nearby moving agents. Due to the lack of other agents’ dynamics and control policies, deep neural network (DNN) models are often employed for trajectory forecasting tasks. Although there exists an extensive literature on improving the accuracy of these models, there is a very limited number of works studying their robustness against adversarially crafted input trajectories. To bridge this gap, in this paper, we propose a targeted adversarial attack against DNN models for trajectory forecasting tasks. We call the proposed attack TA4TP for Targeted adversarial Attack for Trajectory Prediction. Our approach generates adversarial input trajectories that are capable of fooling DNN models into predicting user-specified target/desired trajectories. Our attack relies on solving a nonlinear constrained optimization problem where the objective function captures the deviation of the  predicted trajectory from a target one while the constraints model physical requirements that the adversarial input should satisfy. The latter ensures that the inputs look natural and they are safe to execute (e.g., they are close to nominal inputs and away from obstacles). We demonstrate the effectiveness of TA4TP on two state-of-the-art DNN models and two datasets. To the best of our knowledge, we propose the first targeted adversarial attack against DNN models used for trajectory forecasting.</dcterms:abstract>
        <dc:date>2023-06-06</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>proceedings.mlr.press</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.mlr.press/v211/tan23a.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:32:36</dcterms:dateSubmitted>
        <dc:description>ISSN: 2640-3498</dc:description>
        <bib:pages>431-444</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Learning for Dynamics and Control Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5957">
        <z:itemType>attachment</z:itemType>
        <dc:title>Tan et al_2023_Targeted Adversarial Attacks against Neural Network Trajectory Predictors.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.mlr.press/v211/tan23a/tan23a.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:32:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/1&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/3&quot;&gt;Problem Formulation&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/3&quot;&gt;Trajectory Prediction via Deep Neural Networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/4&quot;&gt;Targeted Adversarial Attack Formulation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/5&quot;&gt;Proposed Targeted Adversarial Attack for Trajectory Prediction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/6&quot;&gt;Experiments&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/6&quot;&gt;Experimental Setup&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/7&quot;&gt;Evaluation of TA4TP&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_P632RA96/10&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/abstract/document/10505805">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1558-0016"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uhlemann</foaf:surname>
                        <foaf:givenName>Nico</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fent</foaf:surname>
                        <foaf:givenName>Felix</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lienkamp</foaf:surname>
                        <foaf:givenName>Markus</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5955"/>
        <dc:title>Evaluating Pedestrian Trajectory Prediction Methods With Respect to Autonomous Driving</dc:title>
        <dcterms:abstract>In this paper, we assess the state of the art in pedestrian trajectory prediction within the context of generating single trajectories, a critical aspect aligning with the requirements in autonomous systems. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. Alongside this, we perform an ablation study to investigate the impact of the observed motion history on prediction performance. To evaluate the scalability of each approach when confronted with varying amounts of agents, the inference time of each model is measured. Following a quantitative analysis, the resulting predictions are compared in a qualitative manner, giving insight into the strengths and weaknesses of current approaches. The results demonstrate that although a constant velocity model (CVM) provides a good approximation of the overall dynamics in the majority of cases, additional features need to be incorporated to reflect common pedestrian behavior observed. Therefore, this study presents a data-driven analysis with the intent to guide the future development of pedestrian trajectory prediction algorithms.</dcterms:abstract>
        <dc:date>2024</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/abstract/document/10505805</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:43:20</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Transactions on Intelligent Transportation Systems</dc:description>
        <bib:pages>1-10</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1558-0016">
        <dc:title>IEEE Transactions on Intelligent Transportation Systems</dc:title>
        <dc:identifier>DOI 10.1109/TITS.2024.3386195</dc:identifier>
        <dc:identifier>ISSN 1558-0016</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5955">
        <z:itemType>attachment</z:itemType>
        <dc:title>Uhlemann et al_2024_Evaluating Pedestrian Trajectory Prediction Methods With Respect to Autonomous Driving.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/ielx7/6979/4358928/10505805.pdf?tp=&amp;arnumber=10505805&amp;isnumber=4358928&amp;ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50LzEwNTA1ODA1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:43:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/CVPR2021/html/Shi_SGCN_Sparse_Graph_Convolution_Network_for_Pedestrian_Trajectory_Prediction_CVPR_2021_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>Liushuai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Le</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Long</foaf:surname>
                        <foaf:givenName>Chengjiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Sanping</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Mo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Niu</foaf:surname>
                        <foaf:givenName>Zhenxing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hua</foaf:surname>
                        <foaf:givenName>Gang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5958"/>
        <dc:title>SGCN: Sparse Graph Convolution Network for Pedestrian Trajectory Prediction</dc:title>
        <dc:date>2021</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SGCN</z:shortTitle>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2021/html/Shi_SGCN_Sparse_Graph_Convolution_Network_for_Pedestrian_Trajectory_Prediction_CVPR_2021_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:57:47</dcterms:dateSubmitted>
        <bib:pages>8994-9003</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5958">
        <z:itemType>attachment</z:itemType>
        <dc:title>Shi et al_2021_SGCN - Sparse Graph Convolution Network for Pedestrian Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_SGCN_Sparse_Graph_Convolution_Network_for_Pedestrian_Trajectory_Prediction_CVPR_2021_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:57:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-3-031-20047-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-3-031-20047-2</dc:identifier>
                <dc:title>Computer Vision – ECCV 2022</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-20047-2_27</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Nature Switzerland</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohamed</foaf:surname>
                        <foaf:givenName>Abduallah</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhu</foaf:surname>
                        <foaf:givenName>Deyao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vu</foaf:surname>
                        <foaf:givenName>Warren</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elhoseiny</foaf:surname>
                        <foaf:givenName>Mohamed</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Claudel</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Avidan</foaf:surname>
                        <foaf:givenName>Shai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brostow</foaf:surname>
                        <foaf:givenName>Gabriel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cissé</foaf:surname>
                        <foaf:givenName>Moustapha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farinella</foaf:surname>
                        <foaf:givenName>Giovanni Maria</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hassner</foaf:surname>
                        <foaf:givenName>Tal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <link:link rdf:resource="#item_5959"/>
        <dc:title>Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation</dc:title>
        <dcterms:abstract>Best-of-N (BoN) Average Displacement Error (ADE)/ Final Displacement Error (FDE) is the most used metric for evaluating trajectory prediction models. Yet, the BoN does not quantify the whole generated samples, resulting in an incomplete view of the model’s prediction quality and performance. We propose a new metric, Average Mahalanobis Distance (AMD) to tackle this issue. AMD is a metric that quantifies how close the whole generated samples are to the ground truth. We also introduce the Average Maximum Eigenvalue (AMV) metric that quantifies the overall spread of the predictions. Our metrics are validated empirically by showing that the ADE/FDE is not sensitive to distribution shifts, giving a biased sense of accuracy, unlike the AMD/AMV metrics. We introduce the usage of Implicit Maximum Likelihood Estimation (IMLE) as a replacement for traditional generative models to train our model, Social-Implicit. IMLE training mechanism aligns with AMD/AMV objective of predicting trajectories that are close to the ground truth with a tight spread. Social-Implicit is a memory efficient deep model with only 5.8K parameters that runs in real time of 580 Hz and achieves competitive results (Code: https://github.com/abduallahmohamed/Social-Implicit/).</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Social-Implicit</z:shortTitle>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <bib:pages>463-479</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5959">
        <z:itemType>attachment</z:itemType>
        <dc:title>Mohamed et al_2022_Social-Implicit - Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2203.03057</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-10 16:18:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_3W8JGADS/1&quot;&gt;Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content_CVPR_2020/html/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohamed</foaf:surname>
                        <foaf:givenName>Abduallah</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qian</foaf:surname>
                        <foaf:givenName>Kun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elhoseiny</foaf:surname>
                        <foaf:givenName>Mohamed</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Claudel</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5956"/>
        <dc:title>Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</dc:title>
        <dc:date>2020</dc:date>
        <z:shortTitle>Social-STGCNN</z:shortTitle>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content_CVPR_2020/html/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:58:19</dcterms:dateSubmitted>
        <bib:pages>14424-14432</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5956">
        <z:itemType>attachment</z:itemType>
        <dc:title>Mohamed et al_2020_Social-STGCNN - A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://openaccess.thecvf.com/content_CVPR_2020/papers/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:58:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2012.01526">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mangalam</foaf:surname>
                        <foaf:givenName>Karttikeya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>An</foaf:surname>
                        <foaf:givenName>Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girase</foaf:surname>
                        <foaf:givenName>Harshayu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malik</foaf:surname>
                        <foaf:givenName>Jitendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_5964"/>
        <link:link rdf:resource="#item_5960"/>
        <dc:title>From Goals, Waypoints &amp; Paths To Long Term Human Trajectory Forecasting</dc:title>
        <dcterms:abstract>Human trajectory forecasting is an inherently multi-modal problem. Uncertainty in future trajectories stems from two sources: (a) sources that are known to the agent but unknown to the model, such as long term goals and (b)sources that are unknown to both the agent &amp; the model, such as intent of other agents &amp; irreducible randomness indecisions. We propose to factorize this uncertainty into its epistemic &amp; aleatoric sources. We model the epistemic un-certainty through multimodality in long term goals and the aleatoric uncertainty through multimodality in waypoints&amp; paths. To exemplify this dichotomy, we also propose a novel long term trajectory forecasting setting, with prediction horizons upto a minute, an order of magnitude longer than prior works. Finally, we presentY-net, a scene com-pliant trajectory forecasting network that exploits the pro-posed epistemic &amp; aleatoric structure for diverse trajectory predictions across long prediction horizons.Y-net significantly improves previous state-of-the-art performance on both (a) The well studied short prediction horizon settings on the Stanford Drone &amp; ETH/UCY datasets and (b) The proposed long prediction horizon setting on the re-purposed Stanford Drone &amp; Intersection Drone datasets.</dcterms:abstract>
        <dc:date>2020-12-02</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2012.01526</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-22 14:12:33</dcterms:dateSubmitted>
        <dc:description>arXiv:2012.01526 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2012.01526</dc:identifier>
        <prism:number>arXiv:2012.01526</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_5964">
       <rdf:value>Comment: 14 pages, 7 figures (including 2 GIFs)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_5960">
        <z:itemType>attachment</z:itemType>
        <dc:title>Mangalam et al_2020_From Goals, Waypoints &amp; Paths To Long Term Human Trajectory Forecasting.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2012.01526.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-22 14:12:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/1&quot;&gt;1 . Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/2&quot;&gt;2 . Related Works&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/2&quot;&gt;2.1 . Unimodal Forecasting&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/3&quot;&gt;2.2 . Multimodality through Generative Modeling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/3&quot;&gt;2.3 . Multimodality through spatial probability estimates&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/3&quot;&gt;3 . Proposed Method&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/3&quot;&gt;3.1 . Y-net Sub-Networks&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/3&quot;&gt;3.1.1 Trajectory on Scene Heatmap Representation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/4&quot;&gt;3.1.2 Trajectory on Scene Heatmap Encoder&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/4&quot;&gt;3.1.3 Goal &amp;amp;amp; Waypoint Heatmap Decoder&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/5&quot;&gt;3.1.4 Trajectory Heatmap Decoder&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/5&quot;&gt;3.2 . Non-parametric Distribution Sampling&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/5&quot;&gt;3.2.1 Test-Time Sampling Trick (TTST)&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/5&quot;&gt;3.2.2 Conditioned Waypoint sampling&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/6&quot;&gt;3.3 . Loss Function&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/6&quot;&gt;4 . Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/6&quot;&gt;4.1 . Datasets&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/7&quot;&gt;4.2 . Implementation Details&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/7&quot;&gt;4.2.1 Segmentation Model Implementation Details&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/8&quot;&gt;4.2.2 Y-net Implementation Details&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/9&quot;&gt;4.3 . Metrics&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/9&quot;&gt;4.4 . Baseline models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/9&quot;&gt;4.5 . Short Term Forecasting Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/9&quot;&gt;4.5.1 Stanford Drone Results&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/10&quot;&gt;4.5.2 ETH &amp;amp;amp; UCY Results&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/10&quot;&gt;4.6 . Long Term Forecasting Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/10&quot;&gt;4.6.1 Forecasting Results&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/11&quot;&gt;4.6.2 Conditoned Waypoint Sampling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/11&quot;&gt;4.6.3 Test-Time Sampling Trick&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/11&quot;&gt;4.6.4 Varying Prediction Horizon&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/11&quot;&gt;4.6.5 Varying Ka&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/12&quot;&gt;4.6.6 Qualitative Results&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_LRSIKFXZ/12&quot;&gt;5 . Conclusion&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-3-030-58536-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-3-030-58536-5</dc:identifier>
                <dc:title>Computer Vision – ECCV 2020</dc:title>
                <dc:identifier>DOI 10.1007/978-3-030-58536-5_45</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mangalam</foaf:surname>
                        <foaf:givenName>Karttikeya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girase</foaf:surname>
                        <foaf:givenName>Harshayu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agarwal</foaf:surname>
                        <foaf:givenName>Shreyas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Kuan-Hui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Adeli</foaf:surname>
                        <foaf:givenName>Ehsan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malik</foaf:surname>
                        <foaf:givenName>Jitendra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gaidon</foaf:surname>
                        <foaf:givenName>Adrien</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vedaldi</foaf:surname>
                        <foaf:givenName>Andrea</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bischof</foaf:surname>
                        <foaf:givenName>Horst</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brox</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Frahm</foaf:surname>
                        <foaf:givenName>Jan-Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <link:link rdf:resource="#item_5963"/>
        <dc:title>It Is Not the Journey But the Destination: Endpoint Conditioned Trajectory Prediction</dc:title>
        <dcterms:abstract>Human trajectory forecasting with multiple socially interacting agents is of critical importance for autonomous navigation in human environments, e.g., for self-driving cars and social robots. In this work, we present Predicted Endpoint Conditioned Network (PECNet) for flexible human trajectory prediction. PECNet infers distant trajectory endpoints to assist in long-range multi-modal trajectory prediction. A novel non-local social pooling layer enables PECNet to infer diverse yet socially compliant trajectories. Additionally, we present a simple “truncation-trick” for improving diversity and multi-modal trajectory prediction performance. We show that PECNet improves state-of-the-art performance on the Stanford Drone trajectory prediction benchmark by $${\sim }20.9\%$$∼20.9%and on the ETH/UCY benchmark by $${\sim }40.8\%$$∼40.8%(Code available at project homepage: https://karttikeya.github.io/publication/htf/).</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>It Is Not the Journey But the Destination</z:shortTitle>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <bib:pages>759-776</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5963">
        <z:itemType>attachment</z:itemType>
        <dc:title>Mangalam et al_2020_It Is Not the Journey But the Destination - Endpoint Conditioned Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2004.02025</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:59:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_LLH87KIC/1&quot;&gt;It is not the Journey but the Destination:  Endpoint Conditioned Trajectory Prediction&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2312.04479">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Zhongchang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Robin</foaf:surname>
                        <foaf:givenName>Marion</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vasishta</foaf:surname>
                        <foaf:givenName>Pavan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5961"/>
        <dc:title>GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian Trajectory Prediction</dc:title>
        <dcterms:abstract>Pedestrian trajectory prediction, vital for selfdriving cars and socially-aware robots, is complicated due to intricate interactions between pedestrians, their environment, and other Vulnerable Road Users. This paper presents GSGFormer, an innovative generative model adept at predicting pedestrian trajectories by considering these complex interactions and offering a plethora of potential modal behaviors. We incorporate a heterogeneous graph neural network to capture interactions between pedestrians, semantic maps, and potential destinations. The Transformer module extracts temporal features, while our novel CVAE-Residual-GMM module promotes diverse behavioral modality generation. Through evaluations on multiple public datasets, GSGFormer not only outperforms leading methods with ample data but also remains competitive when data is limited.</dcterms:abstract>
        <dc:date>2023-12-07</dc:date>
        <z:shortTitle>GSGFormer</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2312.04479</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-22 14:14:25</dcterms:dateSubmitted>
        <dc:description>arXiv:2312.04479 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2312.04479</dc:identifier>
        <prism:number>arXiv:2312.04479</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5961">
        <z:itemType>attachment</z:itemType>
        <dc:title>Luo et al_2023_GSGFormer - Generative Social Graph Transformer for Multimodal Pedestrian Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2312.04479.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-05-22 14:14:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/1&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/1&quot;&gt;Related Work&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/1&quot;&gt;Sequence modelling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/1&quot;&gt;Social forces modelling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/2&quot;&gt;Multi-modal output&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/2&quot;&gt;Theoretical Approach&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/2&quot;&gt;Problem formulation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/2&quot;&gt;Proposed model&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/2&quot;&gt;Agent State Encoders&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/3&quot;&gt;Map Encoder&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/3&quot;&gt;Goal Encoder&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/3&quot;&gt;Social Graph Attention Neural Network&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/3&quot;&gt;Temporal Transformer Model&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/3&quot;&gt;CVAE-Residual-GMM Module&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/3&quot;&gt;Training Strategy&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/4&quot;&gt;Loss function&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/4&quot;&gt;Experiments and Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/4&quot;&gt;Datasets&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/4&quot;&gt;Experimental Setup&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/4&quot;&gt;Baselines and Metrics&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/5&quot;&gt;Implementation details&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/5&quot;&gt;Quantitative Results&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/5&quot;&gt;Qualitative Results&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/6&quot;&gt;Ablation Study&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/6&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_MCDFTIGR/7&quot;&gt;References&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Graph-Based_Spatial_Transformer_With_Memory_Replay_for_Multi-Future_Pedestrian_Trajectory_CVPR_2022_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Lihuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pagnucco</foaf:surname>
                        <foaf:givenName>Maurice</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Yang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5965"/>
        <dc:title>Graph-Based Spatial Transformer With Memory Replay for Multi-Future Pedestrian Trajectory Prediction</dc:title>
        <dc:date>2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/html/Li_Graph-Based_Spatial_Transformer_With_Memory_Replay_for_Multi-Future_Pedestrian_Trajectory_CVPR_2022_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:56:11</dcterms:dateSubmitted>
        <bib:pages>2231-2241</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5965">
        <z:itemType>attachment</z:itemType>
        <dc:title>Li et al_2022_Graph-Based Spatial Transformer With Memory Replay for Multi-Future Pedestrian Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Graph-Based_Spatial_Transformer_With_Memory_Replay_for_Multi-Future_Pedestrian_Trajectory_CVPR_2022_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:56:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ojs.aaai.org/index.php/AAAI/article/view/19933">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2374-3468"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duan</foaf:surname>
                        <foaf:givenName>Jinghai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Le</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Long</foaf:surname>
                        <foaf:givenName>Chengjiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Sanping</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Fang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>Liushuai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hua</foaf:surname>
                        <foaf:givenName>Gang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5962"/>
        <dc:title>Complementary Attention Gated Network for Pedestrian Trajectory Prediction</dc:title>
        <dcterms:abstract>Pedestrian trajectory prediction is crucial in many practical applications due to the diversity of pedestrian movements, such as social interactions and individual motion behaviors. With similar observable trajectories and social environments, different pedestrians may make completely different future decisions. However, most existing methods only focus on the frequent modal of the trajectory and thus are difficult to generalize to the peculiar scenario, which leads to the decline of the multimodal fitting ability when facing similar scenarios. In this paper, we propose a complementary attention gated network (CAGN) for pedestrian trajectory prediction, in which a dual-path architecture including normal and inverse attention is proposed to capture both frequent and peculiar modals in spatial and temporal patterns, respectively. Specifically, a complementary block is proposed to guide normal and inverse attention, which are then be summed with learnable weights to get attention features by a gated network. Finally, multiple trajectory distributions are estimated based on the fused spatio-temporal attention features due to the multimodality of future trajectory. Experimental results on benchmark datasets, i.e., the ETH, and the UCY, demonstrate that our method outperforms state-of-the-art methods by 13.8% in Average Displacement Error (ADE) and 10.4% in Final Displacement Error (FDE). Code will be available at https://github.com/jinghaiD/CAGN</dcterms:abstract>
        <dc:date>2022-06-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ojs.aaai.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ojs.aaai.org/index.php/AAAI/article/view/19933</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:57:14</dcterms:dateSubmitted>
        <dc:rights>Copyright (c) 2022 Association for the Advancement of Artificial Intelligence</dc:rights>
        <dc:description>Number: 1</dc:description>
        <bib:pages>542-550</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2374-3468">
        <prism:volume>36</prism:volume>
        <dc:title>Proceedings of the AAAI Conference on Artificial Intelligence</dc:title>
        <dc:identifier>DOI 10.1609/aaai.v36i1.19933</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2374-3468</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_5962">
        <z:itemType>attachment</z:itemType>
        <dc:title>Duan et al_2022_Complementary Attention Gated Network for Pedestrian Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ojs.aaai.org/index.php/AAAI/article/download/19933/19692</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:57:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/ICCV2021/html/Dendorfer_MG-GAN_A_Multi-Generator_Model_Preventing_Out-of-Distribution_Samples_in_Pedestrian_Trajectory_ICCV_2021_paper.html?ref=https://githubhelp.com">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dendorfer</foaf:surname>
                        <foaf:givenName>Patrick</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Elflein</foaf:surname>
                        <foaf:givenName>Sven</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leal-Taixé</foaf:surname>
                        <foaf:givenName>Laura</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5967"/>
        <dc:title>MG-GAN: A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction</dc:title>
        <dc:date>2021</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>MG-GAN</z:shortTitle>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/ICCV2021/html/Dendorfer_MG-GAN_A_Multi-Generator_Model_Preventing_Out-of-Distribution_Samples_in_Pedestrian_Trajectory_ICCV_2021_paper.html?ref=https://githubhelp.com</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:29:02</dcterms:dateSubmitted>
        <bib:pages>13158-13167</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF International Conference on Computer Vision</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5967">
        <z:itemType>attachment</z:itemType>
        <dc:title>Dendorfer et al_2021_MG-GAN - A Multi-Generator Model Preventing Out-of-Distribution Samples in Pedestrian Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/ICCV2021/papers/Dendorfer_MG-GAN_A_Multi-Generator_Model_Preventing_Out-of-Distribution_Samples_in_Pedestrian_Trajectory_ICCV_2021_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:29:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://proceedings.mlr.press/v205/cao23a.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings of The 6th Conference on Robot Learning</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
           <foaf:Organization><foaf:name>PMLR</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Yulong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Danfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weng</foaf:surname>
                        <foaf:givenName>Xinshuo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>Zhuoqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anandkumar</foaf:surname>
                        <foaf:givenName>Anima</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Chaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pavone</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5966"/>
        <link:link rdf:resource="#item_5968"/>
        <dc:title>Robust Trajectory Prediction against Adversarial Attacks</dc:title>
        <dcterms:abstract>Trajectory prediction using deep neural networks (DNNs) is an essential component of autonomous driving (AD) systems.  However, these methods are vulnerable to adversarial attacks, leading to serious consequences such as collisions. In this work, we identify two key ingredients to defend trajectory prediction models against adversarial attacks including (1) designing effective adversarial training methods and (2) adding domain-specific data augmentation to mitigate the performance degradation on clean data. We demonstrate that our method is able to improve the performance by 46% on adversarial data and at the cost of only 3% performance degradation on clean data, compared to the model trained with clean data. Additionally, compared to existing robust methods, our method can improve performance by 21% on adversarial examples and 9% on clean data. Our robust model is evaluated with a planner to study its downstream impacts. We demonstrate that our model can significantly reduce the severe accident rates (e.g., collisions and off-road driving).</dcterms:abstract>
        <dc:date>2023-03-06</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>proceedings.mlr.press</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.mlr.press/v205/cao23a.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:31:54</dcterms:dateSubmitted>
        <dc:description>ISSN: 2640-3498</dc:description>
        <bib:pages>128-137</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
               <dc:title>Conference on Robot Learning</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5966">
        <z:itemType>attachment</z:itemType>
        <dc:title>Cao et al_2023_Robust Trajectory Prediction against Adversarial Attacks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.mlr.press/v205/cao23a/cao23a.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:31:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/1&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/3&quot;&gt;Related Work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/3&quot;&gt;Preliminaries and Formulation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/4&quot;&gt;RobustTraj: Robust Trajectory Prediction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/6&quot;&gt;Experiments and Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/6&quot;&gt;Experimental setup&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/6&quot;&gt;Main results &lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/7&quot;&gt;Component analysis&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/8&quot;&gt;Limitations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_BDAKYS44/8&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <z:Attachment rdf:about="#item_5968">
        <z:itemType>attachment</z:itemType>
        <dc:title>Cao et al_2023_Robust Trajectory Prediction against Adversarial Attacks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.mlr.press/v205/cao23a/cao23a-supp.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:31:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/1&quot;&gt;Method and Implementations&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/1&quot;&gt;Adversarial Attack on Trajectory Prediction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/2&quot;&gt;Adversarial Training on Generative Models&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/2&quot;&gt;Data Augmentation with Dynamic Model&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/3&quot;&gt;MPC-based Planner&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/4&quot;&gt;Experiment and Results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/4&quot;&gt;More details on Experimental Setup&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8SVDMTNT/5&quot;&gt;Main Results&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-3-031-20065-6">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-3-031-20065-6</dc:identifier>
                <dc:title>Computer Vision – ECCV 2022</dc:title>
                <dc:identifier>DOI 10.1007/978-3-031-20065-6_3</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Nature Switzerland</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Yulong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiao</foaf:surname>
                        <foaf:givenName>Chaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Anandkumar</foaf:surname>
                        <foaf:givenName>Anima</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Danfei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pavone</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Avidan</foaf:surname>
                        <foaf:givenName>Shai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brostow</foaf:surname>
                        <foaf:givenName>Gabriel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cissé</foaf:surname>
                        <foaf:givenName>Moustapha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farinella</foaf:surname>
                        <foaf:givenName>Giovanni Maria</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hassner</foaf:surname>
                        <foaf:givenName>Tal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <link:link rdf:resource="#item_5969"/>
        <dc:title>AdvDO: Realistic Adversarial Attacks for Trajectory Prediction</dc:title>
        <dcterms:abstract>Trajectory prediction is essential for autonomous vehicles (AVs) to plan correct and safe driving behaviors. While many prior works aim to achieve higher prediction accuracy, few study the adversarial robustness of their methods. To bridge this gap, we propose to study the adversarial robustness of data-driven trajectory prediction systems. We devise an optimization-based adversarial attack framework that leverages a carefully-designed differentiable dynamic model to generate realistic adversarial trajectories. Empirically, we benchmark the adversarial robustness of state-of-the-art prediction models and show that our attack increases the prediction error for both general metrics and planning-aware metrics by more than 50% and 37%. We also show that our attack can lead an AV to drive off road or collide into other vehicles in simulation. Finally, we demonstrate how to mitigate the adversarial attacks using an adversarial training scheme (Our project website is at https://robustav.github.io/RobustPred).</dcterms:abstract>
        <dc:date>2022</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>AdvDO</z:shortTitle>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <bib:pages>36-52</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5969">
        <z:itemType>attachment</z:itemType>
        <dc:title>Cao et al_2022_AdvDO - Realistic Adversarial Attacks for Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://authors.library.caltech.edu/118546/1/2209.08744.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 07:31:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://openaccess.thecvf.com/content/CVPR2022/html/Bae_Non-Probability_Sampling_Network_for_Stochastic_Human_Trajectory_Prediction_CVPR_2022_paper.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bae</foaf:surname>
                        <foaf:givenName>Inhwan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Park</foaf:surname>
                        <foaf:givenName>Jin-Hwi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jeon</foaf:surname>
                        <foaf:givenName>Hae-Gon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5970"/>
        <dc:title>Non-Probability Sampling Network for Stochastic Human Trajectory Prediction</dc:title>
        <dc:date>2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>openaccess.thecvf.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/html/Bae_Non-Probability_Sampling_Network_for_Stochastic_Human_Trajectory_Prediction_CVPR_2022_paper.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:56:52</dcterms:dateSubmitted>
        <bib:pages>6477-6487</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5970">
        <z:itemType>attachment</z:itemType>
        <dc:title>Bae et al_2022_Non-Probability Sampling Network for Stochastic Human Trajectory Prediction.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022/papers/Bae_Non-Probability_Sampling_Network_for_Stochastic_Human_Trajectory_Prediction_CVPR_2022_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2024-06-07 06:56:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-94-010-1163-1%20978-94-010-1161-7">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <dc:identifier>ISBN 978-94-010-1163-1 978-94-010-1161-7</dc:identifier>
                <dc:title>Modern Uses of Multiple-Valued Logic</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Dordrecht</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer Netherlands</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dunn</foaf:surname>
                        <foaf:givenName>J. Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Epstein</foaf:surname>
                        <foaf:givenName>George</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belnap</foaf:surname>
                        <foaf:givenName>Nuel D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>A Useful Four-Valued Logic</dc:title>
        <dc:date>1977</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-94-010-1161-7_2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-28 11:04:25</dcterms:dateSubmitted>
        <dc:description>DOI: 10.1007/978-94-010-1161-7_2</dc:description>
        <bib:pages>5-37</bib:pages>
    </bib:BookSection>
    <bib:Article rdf:about="https://www.sae.org/content/12-08-01-0003">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2574-0741,%202574-075X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belmecheri</foaf:surname>
                        <foaf:givenName>Nassim</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gotlieb</foaf:surname>
                        <foaf:givenName>Arnaud</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lazaar</foaf:surname>
                        <foaf:givenName>Nadjib</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Spieker</foaf:surname>
                        <foaf:givenName>Helge</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Toward Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations</dc:title>
        <dcterms:abstract>&lt;div&gt;Understanding driving scenes and communicating automated vehicle decisions are
                    key requirements for trustworthy automated driving. In this article, we
                    introduce the qualitative explainable graph (QXG), which is a unified symbolic
                    and qualitative representation for scene understanding in urban mobility. The
                    QXG enables interpreting an automated vehicle’s environment using sensor data
                    and machine learning models. It utilizes spatiotemporal graphs and qualitative
                    constraints to extract scene semantics from raw sensor inputs, such as LiDAR and
                    camera data, offering an interpretable scene model. A QXG can be incrementally
                    constructed in real-time, making it a versatile tool for in-vehicle explanations
                    across various sensor types. Our research showcases the potential of QXG,
                    particularly in the context of automated driving, where it can rationalize
                    decisions by linking the graph with observed actions. These explanations can
                    serve diverse purposes, from informing passengers and alerting vulnerable road
                    users to enabling post hoc analysis of prior behaviors.&lt;/div&gt;</dcterms:abstract>
        <dc:date>2024-7-12</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.sae.org/content/12-08-01-0003</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-09 07:57:02</dcterms:dateSubmitted>
        <bib:pages>12-08-01-0003</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2574-0741,%202574-075X">
        <prism:volume>8</prism:volume>
        <dc:title>SAE International Journal of Connected and Automated Vehicles</dc:title>
        <dc:identifier>DOI 10.4271/12-08-01-0003</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Connected and Automated Vehicles</dcterms:alternative>
        <dc:identifier>ISSN 2574-0741, 2574-075X</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="http://arxiv.org/abs/2401.10443">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wan</foaf:surname>
                        <foaf:givenName>Ziwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huai</foaf:surname>
                        <foaf:givenName>Yuqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Yuntianyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garcia</foaf:surname>
                        <foaf:givenName>Joshua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Qi Alfred</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_6128"/>
        <link:link rdf:resource="#item_6127"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Software Engineering</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Towards Automated Driving Violation Cause Analysis in Scenario-Based Testing for Autonomous Driving Systems</dc:title>
        <dcterms:abstract>The rapid advancement of Autonomous Vehicles (AVs), exemplified by companies like Waymo and Cruise offering 24/7 paid taxi services, highlights the paramount importance of ensuring AVs' compliance with various policies, such as safety regulations, traffic rules, and mission directives. Despite significant progress in the development of Autonomous Driving System (ADS) testing tools, there has been a notable absence of research on attributing the causes of driving violations. Counterfactual causality analysis has emerged as a promising approach for identifying the root cause of program failures. While it has demonstrated effectiveness in pinpointing error-inducing inputs, its direct application to the AV context to determine which computation result, generated by which component, serves as the root cause poses a considerable challenge. A key obstacle lies in our inability to straightforwardly eliminate the influence of a specific internal message to establish the causal relationship between the output of each component and a system-level driving violation. In this work, we propose a novel driving violation cause analysis (DVCA) tool. We design idealized component substitutes to enable counterfactual analysis of ADS components by leveraging the unique opportunity provided by the simulation. We evaluate our tool on a benchmark with real bugs and injected faults. The results show that our tool can achieve perfect component-level attribution accuracy (100%) and almost (&gt;98%) perfect message-level accuracy. Our tool can reduce the debugging scope from hundreds of complicated interdependent messages to one single computation result generated by one component.</dcterms:abstract>
        <dc:date>2024-01-19</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2401.10443</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-07 11:00:19</dcterms:dateSubmitted>
        <dc:description>arXiv:2401.10443 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2401.10443</dc:identifier>
        <prism:number>arXiv:2401.10443</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_6128">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2401.10443v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-07 11:00:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_6127">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2401.10443</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-01-07 11:00:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2403.13778">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bahari</foaf:surname>
                        <foaf:givenName>Mohammadhossein</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saadatnejad</foaf:surname>
                        <foaf:givenName>Saeed</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farsangi</foaf:surname>
                        <foaf:givenName>Amirhossein Asgari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moosavi-Dezfooli</foaf:surname>
                        <foaf:givenName>Seyed-Mohsen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alahi</foaf:surname>
                        <foaf:givenName>Alexandre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_6142"/>
        <link:link rdf:resource="#item_6141"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Robotics</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Certified Human Trajectory Prediction</dc:title>
        <dcterms:abstract>Trajectory prediction plays an essential role in autonomous vehicles. While numerous strategies have been developed to enhance the robustness of trajectory prediction models, these methods are predominantly heuristic and do not offer guaranteed robustness against adversarial attacks and noisy observations. In this work, we propose a certification approach tailored for the task of trajectory prediction. To this end, we address the inherent challenges associated with trajectory prediction, including unbounded outputs, and mutli-modality, resulting in a model that provides guaranteed robustness. Furthermore, we integrate a denoiser into our method to further improve the performance. Through comprehensive evaluations, we demonstrate the effectiveness of the proposed technique across various baselines and using standard trajectory prediction datasets. The code will be made available online: https://s-attack.github.io/</dcterms:abstract>
        <dc:date>2024-03-20</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2403.13778</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-05 07:11:55</dcterms:dateSubmitted>
        <dc:description>arXiv:2403.13778 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2403.13778</dc:identifier>
        <prism:number>arXiv:2403.13778</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_6142">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2403.13778v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-05 07:11:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_6141">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2403.13778</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-05 07:12:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/10528911/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2379-8904,%202379-8858"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fu</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Kun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Chuchu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yuhang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Jin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Diange</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Summary and Reflections on Pedestrian Trajectory Prediction in the Field of Autonomous Driving</dc:title>
        <dc:date>2024</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10528911/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-04-01 14:22:55</dcterms:dateSubmitted>
        <dc:rights>https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html</dc:rights>
        <bib:pages>1-33</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2379-8904,%202379-8858">
        <dc:title>IEEE Transactions on Intelligent Vehicles</dc:title>
        <dc:identifier>DOI 10.1109/TIV.2024.3399327</dc:identifier>
        <dcterms:alternative>IEEE Trans. Intell. Veh.</dcterms:alternative>
        <dc:identifier>ISSN 2379-8904, 2379-8858</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="urn:isbn:978-1-5386-3081-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-5386-3081-5</dc:identifier>
                <dc:title>2018 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2018.8462969</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Brisbane, QLD</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Beom-Jin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choi</foaf:surname>
                        <foaf:givenName>Jinyoung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baek</foaf:surname>
                        <foaf:givenName>Christina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Byoung-Tak</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots</dc:title>
        <dc:date>5/2018</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8462969/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-04-01 14:14:45</dcterms:dateSubmitted>
        <bib:pages>7189-7195</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2018 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <rdf:Description rdf:about="https://arxiv.org/abs/1805.11833">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Yuanfu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Panpan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bera</foaf:surname>
                        <foaf:givenName>Aniket</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hsu</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Wee Sun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manocha</foaf:surname>
                        <foaf:givenName>Dinesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>FOS: Computer and information sciences</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Robotics (cs.RO)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PORCA: Modeling and Planning for Autonomous Driving among Many Pedestrians</dc:title>
        <dcterms:abstract>This paper presents a planning system for autonomous driving among many pedestrians. A key ingredient of our approach is PORCA, a pedestrian motion prediction model that accounts for both a pedestrian's global navigation intention and local interactions with the vehicle and other pedestrians. Unfortunately, the autonomous vehicle does not know the pedestrian's intention a priori and requires a planning algorithm that hedges against the uncertainty in pedestrian intentions. Our planning system combines a POMDP algorithm with the pedestrian motion model and runs in near real time. Experiments show that it enables a robot vehicle to drive safely, efficiently, and smoothly among a crowd with a density of nearly one person per square meter.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:shortTitle>PORCA</z:shortTitle>
        <z:libraryCatalog>DOI.org (Datacite)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1805.11833</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-27 15:32:46</dcterms:dateSubmitted>
        <dc:rights>arXiv.org perpetual, non-exclusive license</dc:rights>
        <dc:description>Version Number: 2</dc:description>
        <dc:identifier>DOI 10.48550/ARXIV.1805.11833</dc:identifier>
    </rdf:Description>
    <bib:Article rdf:about="http://link.springer.com/10.1007/s12369-009-0037-z">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1875-4791,%201875-4805"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Foka</foaf:surname>
                        <foaf:givenName>Amalia F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Trahanias</foaf:surname>
                        <foaf:givenName>Panos E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Probabilistic Autonomous Robot Navigation in Dynamic Environments with Human Motion Prediction</dc:title>
        <dc:date>3/2010</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/s12369-009-0037-z</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-27 15:30:43</dcterms:dateSubmitted>
        <dc:rights>http://www.springer.com/tdm</dc:rights>
        <bib:pages>79-94</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1875-4791,%201875-4805">
        <prism:volume>2</prism:volume>
        <dc:title>International Journal of Social Robotics</dc:title>
        <dc:identifier>DOI 10.1007/s12369-009-0037-z</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Int J of Soc Robotics</dcterms:alternative>
        <dc:identifier>ISSN 1875-4791, 1875-4805</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="https://digital-library.theiet.org/content/journals/10.1049/ip-vis_20041147">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1350245X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Valera</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Velastin</foaf:surname>
                        <foaf:givenName>S.A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Intelligent distributed surveillance systems: a review</dc:title>
        <dc:date>2005</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Intelligent distributed surveillance systems</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://digital-library.theiet.org/content/journals/10.1049/ip-vis_20041147</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-27 15:23:53</dcterms:dateSubmitted>
        <bib:pages>192</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1350245X">
        <prism:volume>152</prism:volume>
        <dc:title>IEE Proceedings - Vision, Image, and Signal Processing</dc:title>
        <dc:identifier>DOI 10.1049/ip-vis:20041147</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>IEE Proc., Vis. Image Process.</dcterms:alternative>
        <dc:identifier>ISSN 1350245X</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="urn:isbn:978-1-4673-8026-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4673-8026-3</dc:identifier>
                <dc:title>2016 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2016.7487768</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Stockholm, Sweden</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bera</foaf:surname>
                        <foaf:givenName>Aniket</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Sujeong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Randhavane</foaf:surname>
                        <foaf:givenName>Tanmay</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pratapa</foaf:surname>
                        <foaf:givenName>Srihari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manocha</foaf:surname>
                        <foaf:givenName>Dinesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>GLMP- realtime pedestrian path prediction using global and local movement patterns</dc:title>
        <dc:date>5/2016</dc:date>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/7487768/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-04-02 07:24:02</dcterms:dateSubmitted>
        <bib:pages>5528-5535</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2016 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Collection rdf:about="#collection_149">
       <dc:title>_Unsorted</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_141">
        <dc:title>AI4CCAM Methodology</dc:title>
        <dcterms:hasPart rdf:resource="#collection_142"/>
        <dcterms:hasPart rdf:resource="#collection_143"/>
        <dcterms:hasPart rdf:resource="#collection_144"/>
        <dcterms:hasPart rdf:resource="#collection_145"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_142">
       <dc:title>1 - Problem Specification</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_143">
       <dc:title>2 - Data Engineering</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_144">
       <dc:title>3 - AI Model Design</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_145">
       <dc:title>4 - Evaluation and Verification Phase</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_133">
        <dc:title>CCAM</dc:title>
        <dcterms:hasPart rdf:resource="#collection_146"/>
        <dcterms:hasPart rdf:resource="#collection_117"/>
        <dcterms:hasPart rdf:resource="#collection_139"/>
        <dcterms:hasPart rdf:resource="#collection_147"/>
        <dcterms:hasPart rdf:resource="#collection_135"/>
        <dcterms:hasPart rdf:resource="#collection_136"/>
        <dcterms:hasPart rdf:resource="#collection_131"/>
        <dcterms:hasPart rdf:resource="#collection_134"/>
        <dcterms:hasPart rdf:resource="#collection_150"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_146">
       <dc:title>ADAS</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_117">
        <dc:title>Datasets</dc:title>
        <dcterms:hasPart rdf:resource="#item_5643"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2302.02790"/>
        <dcterms:hasPart rdf:resource="#item_5648"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2004.06320"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1912.04838"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2304.01168"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2112.12610"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1910.03088"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2307.13924"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_139">
       <dc:title>ELSEC</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_147">
       <dc:title>Requirements</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_135">
        <dc:title>Scene Understanding</dc:title>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s11633-018-1126-y"/>
        <dcterms:hasPart rdf:resource="#item_5643"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1807.11546"/>
        <dcterms:hasPart rdf:resource="https://arxiv.org/abs/2208.00249"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2205.04712"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2207.07496"/>
        <dcterms:hasPart rdf:resource="http://dergipark.org.tr/en/doi/10.38002/tuad.1084567"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2312.06352"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/10268664/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2210.15375"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-94-010-1163-1%20978-94-010-1161-7"/>
        <dcterms:hasPart rdf:resource="https://www.sae.org/content/12-08-01-0003"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2401.10443"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_136">
        <dc:title>Trajectory Prediction</dc:title>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_On_Adversarial_Robustness_of_Trajectory_Prediction_for_Autonomous_Vehicles_CVPR_2022_paper.html"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_TrajPAC_Towards_Robustness_Verification_of_Pedestrian_Trajectory_Prediction_Models_ICCV_2023_paper.html"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/WACV2023/html/Zheng_Robustness_of_Trajectory_Prediction_Models_Under_Map-Based_Attacks_WACV_2023_paper.html"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GroupNet_Multiscale_Hypergraph_Neural_Networks_for_Trajectory_Prediction_With_Relational_CVPR_2022_paper.html"/>
        <dcterms:hasPart rdf:resource="https://proceedings.mlr.press/v211/tan23a.html"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/abstract/document/10505805"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/CVPR2021/html/Shi_SGCN_Sparse_Graph_Convolution_Network_for_Pedestrian_Trajectory_Prediction_CVPR_2021_paper.html"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-031-20047-2"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content_CVPR_2020/html/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.html"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2012.01526"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-030-58536-5"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2312.04479"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Graph-Based_Spatial_Transformer_With_Memory_Replay_for_Multi-Future_Pedestrian_Trajectory_CVPR_2022_paper.html"/>
        <dcterms:hasPart rdf:resource="https://ojs.aaai.org/index.php/AAAI/article/view/19933"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/ICCV2021/html/Dendorfer_MG-GAN_A_Multi-Generator_Model_Preventing_Out-of-Distribution_Samples_in_Pedestrian_Trajectory_ICCV_2021_paper.html?ref=https://githubhelp.com"/>
        <dcterms:hasPart rdf:resource="https://proceedings.mlr.press/v205/cao23a.html"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-031-20065-6"/>
        <dcterms:hasPart rdf:resource="https://openaccess.thecvf.com/content/CVPR2022/html/Bae_Non-Probability_Sampling_Network_for_Stochastic_Human_Trajectory_Prediction_CVPR_2022_paper.html"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2403.13778"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/10528911/"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-5386-3081-5"/>
        <dcterms:hasPart rdf:resource="https://arxiv.org/abs/1805.11833"/>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1007/s12369-009-0037-z"/>
        <dcterms:hasPart rdf:resource="https://digital-library.theiet.org/content/journals/10.1049/ip-vis_20041147"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4673-8026-3"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_131">
       <dc:title>Use Cases</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_134">
        <dc:title>User Acceptance</dc:title>
        <dcterms:hasPart rdf:resource="#item_5714"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_150">
       <dc:title>VRUs</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_122">
        <dc:title>Key Requirements for Trustworthy AI</dc:title>
        <dcterms:hasPart rdf:resource="#collection_124"/>
        <dcterms:hasPart rdf:resource="#collection_125"/>
        <dcterms:hasPart rdf:resource="#collection_126"/>
        <dcterms:hasPart rdf:resource="#collection_127"/>
        <dcterms:hasPart rdf:resource="#collection_128"/>
        <dcterms:hasPart rdf:resource="#collection_129"/>
        <dcterms:hasPart rdf:resource="#collection_130"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_124">
       <dc:title>1 - Human Agency and Oversight</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_125">
        <dc:title>2 - Technical Robustness and Safety</dc:title>
        <dcterms:hasPart rdf:resource="#collection_148"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_148">
        <dc:title>Testing</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1809.04843"/>
        <dcterms:hasPart rdf:resource="https://doi.org/10.1109/ITSC55140.2022.9921776"/>
        <dcterms:hasPart rdf:resource="https://openreview.net/forum?id=ZjN2AuXgu1"/>
        <dcterms:hasPart rdf:resource="#item_5931"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_126">
       <dc:title>3 - Privacy and Data Governance</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_127">
        <dc:title>4 - Transparency</dc:title>
        <dcterms:hasPart rdf:resource="#collection_95"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_95">
        <dc:title>Explainable AI</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1910.10045"/>
        <dcterms:hasPart rdf:resource="https://www.aclweb.org/anthology/W19-8403"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1806.00069"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1802.01933"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-4232-2"/>
        <dcterms:hasPart rdf:resource="#item_5663"/>
        <dcterms:hasPart rdf:resource="#item_5665"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-5971-9"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-7516-0"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-4503-8096-6"/>
        <dcterms:hasPart rdf:resource="#item_5684"/>
        <dcterms:hasPart rdf:resource="#item_5704"/>
        <dcterms:hasPart rdf:resource="#item_5714"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_128">
       <dc:title>5 - Diversity, Non-Discrimination and Fairness</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_129">
       <dc:title>6 - Societal and Environmental Well-Being</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_130">
       <dc:title>7 - Accountability</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_162">
       <dc:title>WP1</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_163">
       <dc:title>WP2</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_164">
       <dc:title>WP3</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_165">
       <dc:title>WP4</dc:title>
    </z:Collection>
</rdf:RDF>
